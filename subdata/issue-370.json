{"url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/2076", "repository_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT", "labels_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/2076/labels{/name}", "comments_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/2076/comments", "events_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/2076/events", "html_url": "https://github.com/Significant-Gravitas/Auto-GPT/issues/2076", "id": 1670338514, "node_id": "I_kwDOJKSTjM5jj1fS", "number": 2076, "title": "Failing to use pre-seeded data and/or chunk a large JSON file", "user": {"login": "Explorergt92", "id": 81686492, "node_id": "MDQ6VXNlcjgxNjg2NDky", "avatar_url": "https://avatars.githubusercontent.com/u/81686492?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Explorergt92", "html_url": "https://github.com/Explorergt92", "followers_url": "https://api.github.com/users/Explorergt92/followers", "following_url": "https://api.github.com/users/Explorergt92/following{/other_user}", "gists_url": "https://api.github.com/users/Explorergt92/gists{/gist_id}", "starred_url": "https://api.github.com/users/Explorergt92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Explorergt92/subscriptions", "organizations_url": "https://api.github.com/users/Explorergt92/orgs", "repos_url": "https://api.github.com/users/Explorergt92/repos", "events_url": "https://api.github.com/users/Explorergt92/events{/privacy}", "received_events_url": "https://api.github.com/users/Explorergt92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 5401552627, "node_id": "LA_kwDOJKSTjM8AAAABQfUm8w", "url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/labels/needs%20investigation", "name": "needs investigation", "color": "C3708F", "default": false, "description": ""}, {"id": 5409160395, "node_id": "LA_kwDOJKSTjM8AAAABQmk8yw", "url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/labels/function:%20memory", "name": "function: memory", "color": "E12F75", "default": false, "description": ""}, {"id": 5410513801, "node_id": "LA_kwDOJKSTjM8AAAABQn3jiQ", "url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/labels/AI%20efficacy", "name": "AI efficacy", "color": "3715D0", "default": false, "description": ""}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2023-04-17T03:27:18Z", "updated_at": "2023-04-24T15:53:28Z", "closed_at": null, "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### \u26a0\ufe0f Search for existing issues first \u26a0\ufe0f\r\n\r\n- [X] I have searched the existing issues, and there is no existing issue for my problem\r\n\r\n### GPT-3 or GPT-4\r\n\r\n- [x] I am using Auto-GPT with GPT-3 (GPT-3.5)\r\n- [x] I am using Auto-GPT with GPT-4\r\n- Using GTP-4 as SMART\r\n- Using GPT-3 as FAST\r\n\r\n### Steps to reproduce \ud83d\udd79\r\n\r\nStarted a fresh redis  server in docker and pre-seeded the issues_data.json file to redis with:\r\n\r\n`python data_ingestion.py --f issues_data.json --init --overlap 300 --max_length 3000`\r\n\r\nI verified the size of the memory file `dump.rdb' in the redis container grew ... and it went up from nearly nothing to 64MB.\r\n\r\n'python -m autogpt'\r\n\r\nUsed a design.txt file to tell it how it's supposed to work that contains:\r\n\r\n# Design Document for GitHubIssuesFAQ-Ai\r\n\r\n## Table of Contents\r\n\r\n1. [Introduction](#introduction)\r\n2. [System Overview](#system-overview)\r\n3. [Functional Requirements](#functional-requirements)\r\n4. [Non-Functional Requirements](#non-functional-requirements)\r\n5. [System Architecture](#system-architecture)\r\n6. [Technologies](#technologies)\r\n7. [Testing](#testing)\r\n8. [Conclusion](#conclusion)\r\n\r\n## Introduction\r\n\r\nGitHubIssuesFAQ-Ai is an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues. The AI will read design specifications, follow advice, analyze frequently asked questions, and generate a FAQ based on the most common questions and their answers.\r\n\r\n## System Overview\r\n\r\nThe GitHubIssuesFAQ-Ai will perform the following tasks:\r\n\r\n1. Read `design.txt` and follow its design specifications.\r\n2. Read `advice.txt` and obey it every 10 minutes.\r\n3. Use the information saved in its memory to determine the most frequently asked questions from the repo's issues posts.\r\n4. Determine the best answer to the most frequently asked questions from the issues comments.\r\n5. Write a FAQ and answer the most frequently asked questions.\r\n\r\n## Functional Requirements\r\n\r\n1. **Read design specifications**: The AI will read `design.txt` and follow the design specifications provided in the file.\r\n2. **Follow advice**: The AI will read `advice.txt` and obey the advice provided in the file every 10 minutes.\r\n3. **Analyze frequently asked questions**: The AI will analyze the repo's issues posts and determine the most frequently asked questions.\r\n4. **Determine the best answer**: The AI will analyze the issues comments and determine the best answer to the most frequently asked questions.\r\n5. **Generate FAQ**: The AI will write a FAQ document that answers the most frequently asked questions.\r\n\r\n## Non-Functional Requirements\r\n\r\n1. **Performance**: The AI should be able to handle a large number of issues and comments without significant performance degradation.\r\n2. **Scalability**: The AI should be able to scale to handle an increasing number of issues and comments.\r\n3. **Accuracy**: The AI should accurately identify the most frequently asked questions and their best answers.\r\n4. **Usability**: The generated FAQ should be easy to read and understand by users.\r\n\r\n## System Architecture\r\n\r\nThe GitHubIssuesFAQ-Ai system architecture consists of the following components:\r\n\r\n1. **Data Ingestion**: This component is responsible for reading the `design.txt` and `advice.txt` files and ingesting the repo's issues and comments data.\r\n2. **Data Processing**: This component is responsible for processing the ingested data and determining the most frequently asked questions and their best answers.\r\n3. **FAQ Generation**: This component is responsible for generating the FAQ document based on the most frequently asked questions and their best answers.\r\n4. **Output**: This component is responsible for outputting the generated FAQ document.\r\n\r\n## Technologies\r\n\r\nThe following technologies will be used for the development of GitHubIssuesFAQ-Ai:\r\n\r\n1. **Python**: The AI will be developed using Python programming language.\r\n2. **Natural Language Processing (NLP) libraries**: Libraries such as NLTK, spaCy, and Gensim will be used for text processing and analysis.\r\n3. **GitHub API**: The GitHub API will be used to access the repo's issues and comments data.\r\n\r\n## Testing\r\n\r\nThe GitHubIssuesFAQ-Ai will be tested using the following methods:\r\n\r\n1. **Unit testing**: Unit tests will be written for each component to ensure that they are functioning correctly.\r\n2. **Integration testing**: Integration tests will be written to ensure that the components are working together correctly.\r\n3. **System testing**: The entire system will be tested to ensure that it meets the functional and non-functional requirements.\r\n4. **User Acceptance testing**: The generated FAQ document will be reviewed by users to ensure that it is easy to read and understand.\r\n\r\n## Conclusion\r\n\r\nThe GitHubIssuesFAQ-Ai is an AI designed to autonomously analyze issues posted in the github repository and generate a FAQ document based on the most frequently asked questions and their best answers. The AI will read design specifications, follow advice, analyze frequently asked questions, and generate a FAQ. The system architecture consists of data ingestion, data processing, FAQ generation, and output components. The AI will be developed using Python, NLP libraries, and the GitHub API. The system will be tested using unit, integration, system, and user acceptance testing methods.\r\n\r\n\r\n\r\n### Current behavior \ud83d\ude2f\r\n\r\nAuto-GPT failed to use pre-seeded data at all and went ahead and downloaded the main repo to /Auto-GPT/auto_gpt_workspace folder to gather data. \r\n\r\nAfter trying to prompt it to use the redis memory and failing, I instructed it to 'read issues_data.json' and after 5 minutes of the data it's reading scrolling up the screen I get an error that reads:\r\n\r\n`openai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 7181084 tokens (7181084 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.`\r\n\r\nThen Auto-GPT crashes back to the PS command prompt. \r\n\r\n### Expected behavior \ud83e\udd14\r\n\r\nExpected it to use data pre-seeded into redis memory \r\n\r\nHoped it would be able to use the data from the issues_data.json file directly instead as a work around.\r\n\r\n### Your prompt \ud83d\udcdd\r\n\r\n```yaml\r\nai_goals:\r\n- Read design.txt and follow its design specifications.\r\n- Read advice.txt and obey it every 10 minutes.\r\n- Use the information saved in your memory to determine the most frequently asked questions from the repos issues posts.\r\n- Determine the best answer to the most frequently asked questions from the issues comments.\r\n- Write a FAQ and answer the most frequently asked questions.\r\nai_name: GitHubIssuesFAQ-Ai\r\nai_role: an AI designed to autonomously manage GitHub issues to make it easier for users to find solutions to their issues.\r\n\r\n```\r\n\r\nadvice.txt contains\r\n\r\n1. Use the data saved in your memory as it already has all the JSON data from the repos you are watching.", "reactions": {"url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/2076/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/2076/timeline", "performed_via_github_app": null, "state_reason": "reopened"}