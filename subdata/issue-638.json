{"url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/38", "repository_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT", "labels_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/38/labels{/name}", "comments_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/38/comments", "events_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/38/events", "html_url": "https://github.com/Significant-Gravitas/Auto-GPT/issues/38", "id": 1651048142, "node_id": "I_kwDOJKSTjM5iaP7O", "number": 38, "title": "Improve chunking and chunk handling", "user": {"login": "cometbus", "id": 8890602, "node_id": "MDQ6VXNlcjg4OTA2MDI=", "avatar_url": "https://avatars.githubusercontent.com/u/8890602?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cometbus", "html_url": "https://github.com/cometbus", "followers_url": "https://api.github.com/users/cometbus/followers", "following_url": "https://api.github.com/users/cometbus/following{/other_user}", "gists_url": "https://api.github.com/users/cometbus/gists{/gist_id}", "starred_url": "https://api.github.com/users/cometbus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cometbus/subscriptions", "organizations_url": "https://api.github.com/users/cometbus/orgs", "repos_url": "https://api.github.com/users/cometbus/repos", "events_url": "https://api.github.com/users/cometbus/events{/privacy}", "received_events_url": "https://api.github.com/users/cometbus/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 5272676193, "node_id": "LA_kwDOJKSTjM8AAAABOkanYQ", "url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 28, "created_at": "2023-04-02T18:29:58Z", "updated_at": "2023-05-02T11:29:22Z", "closed_at": null, "author_association": "NONE", "active_lock_reason": null, "body": "AutoGPT keeps exceeding the token limit by like 100 tokens every time I use it before it finishes it's task and can't handle the error\r\nTraceback (most recent call last):\r\n  File \"/root/Auto-GPT/scripts/main.py\", line 199, in <module>\r\n    assistant_reply = chat.chat_with_ai(\r\n  File \"/root/Auto-GPT/scripts/chat.py\", line 64, in chat_with_ai\r\n    response = openai.ChatCompletion.create(\r\n  File \"/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\r\n    return super().create(*args, **kwargs)\r\n  File \"/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\r\n    response, _, api_key = requestor.request(\r\n  File \"/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_requestor.py\", line 226, in request\r\n    resp, got_stream = self._interpret_response(result, stream)\r\n  File \"/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_requestor.py\", line 619, in _interpret_response\r\n    self._interpret_response_line(\r\n  File \"/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_requestor.py\", line 682, in _interpret_response_line\r\n    raise self.handle_error_response(\r\nopenai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 8215 tokens. Please reduce the length of the messages.\r\n\r\nNEXT ACTION:  COMMAND = Error: ARGUMENTS = Invalid JSON\r\nSYSTEM:  Command Error: returned: Unknown command Error:\r\nTraceback (most recent call last):\r\n  File \"/root/Auto-GPT/scripts/main.py\", line 199, in <module>\r\n    assistant_reply = chat.chat_with_ai(\r\n  File \"/root/Auto-GPT/scripts/chat.py\", line 64, in chat_with_ai\r\n    response = openai.ChatCompletion.create(\r\n  File \"/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\r\n    return super().create(*args, **kwargs)\r\n  File \"/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\r\n    response, _, api_key = requestor.request(\r\n  File \"/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_requestor.py\", line 226, in request\r\n    resp, got_stream = self._interpret_response(result, stream)\r\n  File \"/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_requestor.py\", line 619, in _interpret_response\r\n    self._interpret_response_line(\r\n  File \"/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_requestor.py\", line 682, in _interpret_response_line\r\n    raise self.handle_error_response(\r\nopenai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 8215 tokens. Please reduce the length of the messages.\r\n(myenv) (base) root@DESKTOP-S70O6TN:~/Auto-GPT/scripts#", "reactions": {"url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/38/reactions", "total_count": 9, "+1": 9, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/38/timeline", "performed_via_github_app": null, "state_reason": "reopened"}