{"url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/25", "repository_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT", "labels_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/25/labels{/name}", "comments_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/25/comments", "events_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/25/events", "html_url": "https://github.com/Significant-Gravitas/Auto-GPT/issues/25", "id": 1650937620, "node_id": "I_kwDOJKSTjM5iZ08U", "number": 25, "title": "Support using other/local LLMs", "user": {"login": "DataBassGit", "id": 126125195, "node_id": "U_kgDOB4SEiw", "avatar_url": "https://avatars.githubusercontent.com/u/126125195?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DataBassGit", "html_url": "https://github.com/DataBassGit", "followers_url": "https://api.github.com/users/DataBassGit/followers", "following_url": "https://api.github.com/users/DataBassGit/following{/other_user}", "gists_url": "https://api.github.com/users/DataBassGit/gists{/gist_id}", "starred_url": "https://api.github.com/users/DataBassGit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DataBassGit/subscriptions", "organizations_url": "https://api.github.com/users/DataBassGit/orgs", "repos_url": "https://api.github.com/users/DataBassGit/repos", "events_url": "https://api.github.com/users/DataBassGit/events{/privacy}", "received_events_url": "https://api.github.com/users/DataBassGit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 5272676243, "node_id": "LA_kwDOJKSTjM8AAAABOkankw", "url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 71, "created_at": "2023-04-02T12:57:28Z", "updated_at": "2023-05-01T19:24:22Z", "closed_at": null, "author_association": "NONE", "active_lock_reason": null, "body": "You can modify the code to accept a config file as input, and read the Chosen_Model flag to select the appropriate AI model. Here's an example of how to achieve this:\r\n\r\nCreate a sample config file named config.ini:\r\n\r\n[AI]\r\nChosen_Model = gpt-4\r\n\r\nOffload the call_ai_fuction from the ai_functions.py to a separate library. Modify the call_ai_function function to read the model from the config file:\r\n\r\n```\r\nimport configparser\r\n\r\ndef call_ai_function(function, args, description, config_path=\"config.ini\"):\r\n    # Load the configuration file\r\n    config = configparser.ConfigParser()\r\n    config.read(config_path)\r\n\r\n    # Get the chosen model from the config file\r\n    model = config.get(\"AI\", \"Chosen_Model\", fallback=\"gpt-4\")\r\n\r\n    # Parse args to comma separated string\r\n    args = \", \".join(args)\r\n    messages = [\r\n        {\r\n            \"role\": \"system\",\r\n            \"content\": f\"You are now the following python function: ```# {description}\\n{function}```\\n\\nOnly respond with your `return` value.\",\r\n        },\r\n        {\"role\": \"user\", \"content\": args},\r\n    ]\r\n\r\n  # Use different AI APIs based on the chosen model\r\n    if model == \"gpt-4\":\r\n        response = openai.ChatCompletion.create(\r\n            model=model, messages=messages, temperature=0\r\n        )\r\n    elif model == \"some_other_api\":\r\n        # Add code to call another AI API with the appropriate parameters\r\n        response = some_other_api_call(parameters)\r\n    else:\r\n        raise ValueError(f\"Unsupported model: {model}\")\r\n\r\n    return response.choices[0].message[\"content\"]\r\n\r\n```\r\nIn this modified version, the call_ai_function takes an additional parameter config_path which defaults to \"config.ini\". The function reads the config file, retrieves the Chosen_Model value, and uses it as the model for the OpenAI API call. If the Chosen_Model flag is not found in the config file, it defaults to \"gpt-4\".\r\n\r\n the if/elif structure is used to call different AI APIs based on the chosen model from the configuration file. Replace some_other_api with the name of the API you'd like to use, and replace parameters with the appropriate parameters required by that API. You can extend the if/elif structure to include more AI APIs as needed.", "reactions": {"url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/25/reactions", "total_count": 13, "+1": 1, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 10, "rocket": 2, "eyes": 0}, "timeline_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/25/timeline", "performed_via_github_app": null, "state_reason": null}