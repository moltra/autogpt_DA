{"url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/1841", "repository_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT", "labels_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/1841/labels{/name}", "comments_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/1841/comments", "events_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/1841/events", "html_url": "https://github.com/Significant-Gravitas/Auto-GPT/issues/1841", "id": 1669842905, "node_id": "I_kwDOJKSTjM5jh8fZ", "number": 1841, "title": "Prompt overflows aren't handled gracefully", "user": {"login": "tony163163", "id": 6356626, "node_id": "MDQ6VXNlcjYzNTY2MjY=", "avatar_url": "https://avatars.githubusercontent.com/u/6356626?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tony163163", "html_url": "https://github.com/tony163163", "followers_url": "https://api.github.com/users/tony163163/followers", "following_url": "https://api.github.com/users/tony163163/following{/other_user}", "gists_url": "https://api.github.com/users/tony163163/gists{/gist_id}", "starred_url": "https://api.github.com/users/tony163163/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tony163163/subscriptions", "organizations_url": "https://api.github.com/users/tony163163/orgs", "repos_url": "https://api.github.com/users/tony163163/repos", "events_url": "https://api.github.com/users/tony163163/events{/privacy}", "received_events_url": "https://api.github.com/users/tony163163/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 5272676193, "node_id": "LA_kwDOJKSTjM8AAAABOkanYQ", "url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 5410206205, "node_id": "LA_kwDOJKSTjM8AAAABQnkx_Q", "url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/labels/function:%20process%20text", "name": "function: process text", "color": "266F47", "default": false, "description": ""}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 25, "created_at": "2023-04-16T10:28:31Z", "updated_at": "2023-04-29T20:32:13Z", "closed_at": null, "author_association": "NONE", "active_lock_reason": null, "body": "### Duplicates\n\n- [X] I have searched the existing issues\n\n### Steps to reproduce \ud83d\udd79\n\n_No response_\n\n### Current behavior \ud83d\ude2f\n\nWhen using Chinese text, the length increases after encoding, which may cause the number of tokens to exceed 8191 and result in the following error\uff1a\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\tony1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"C:\\Users\\tony1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"E:\\openai\\git\\Auto-GPT\\autogpt\\__main__.py\", line 53, in <module>\r\n    main()\r\n  File \"E:\\openai\\git\\Auto-GPT\\autogpt\\__main__.py\", line 49, in main\r\n    agent.start_interaction_loop()\r\n  File \"E:\\openai\\git\\Auto-GPT\\autogpt\\agent\\agent.py\", line 65, in start_interaction_loop\r\n    assistant_reply = chat_with_ai(\r\n  File \"E:\\openai\\git\\Auto-GPT\\autogpt\\chat.py\", line 85, in chat_with_ai\r\n    else permanent_memory.get_relevant(str(full_message_history[-9:]), 10)\r\n  File \"E:\\openai\\git\\Auto-GPT\\autogpt\\memory\\local.py\", line 122, in get_relevant\r\n    embedding = create_embedding_with_ada(text)\r\n  File \"E:\\openai\\git\\Auto-GPT\\autogpt\\llm_utils.py\", line 136, in create_embedding_with_ada\r\n    return openai.Embedding.create(\r\n  File \"C:\\Users\\tony1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\embedding.py\", line 33, in create\r\n    response = super().create(*args, **kwargs)\r\n  File \"C:\\Users\\tony1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\r\n    response, _, api_key = requestor.request(\r\n  File \"C:\\Users\\tony1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 226, in request\r\n    resp, got_stream = self._interpret_response(result, stream)\r\n  File \"C:\\Users\\tony1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 619, in _interpret_response\r\n    self._interpret_response_line(\r\n  File \"C:\\Users\\tony1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py\", line 682, in _interpret_response_line\r\n    raise self.handle_error_response(\r\nopenai.error.InvalidRequestError: This model's maximum context length is 8191 tokens, however you requested 10681 tokens (10681 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\r\n\r\n\n\n### Expected behavior \ud83e\udd14\n\nIf not encoded, there will be no problem:\r\n\r\nautogpt\\app.py\r\n\r\n130:  google_result = google_search(arguments[\"input\"])\r\n131:  #safe_message = google_result.encode(\"utf-8\", \"ignore\")\r\n132:  return str(google_result)\r\n\n\n### Your prompt \ud83d\udcdd\n\n```yaml\r\n# Paste your prompt here\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/1841/reactions", "total_count": 8, "+1": 8, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/1841/timeline", "performed_via_github_app": null, "state_reason": "reopened"}