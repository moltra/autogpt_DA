{"url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/3670", "repository_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT", "labels_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/3670/labels{/name}", "comments_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/3670/comments", "events_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/3670/events", "html_url": "https://github.com/Significant-Gravitas/Auto-GPT/issues/3670", "id": 1691837682, "node_id": "I_kwDOJKSTjM5k12Ty", "number": 3670, "title": "I made a CLIP vision + GPT-* + Stable Diffusion = Auto-GPT-SD + visual web (image) search. But...", "user": {"login": "zer0int", "id": 132047210, "node_id": "U_kgDOB97hag", "avatar_url": "https://avatars.githubusercontent.com/u/132047210?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zer0int", "html_url": "https://github.com/zer0int", "followers_url": "https://api.github.com/users/zer0int/followers", "following_url": "https://api.github.com/users/zer0int/following{/other_user}", "gists_url": "https://api.github.com/users/zer0int/gists{/gist_id}", "starred_url": "https://api.github.com/users/zer0int/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zer0int/subscriptions", "organizations_url": "https://api.github.com/users/zer0int/orgs", "repos_url": "https://api.github.com/users/zer0int/repos", "events_url": "https://api.github.com/users/zer0int/events{/privacy}", "received_events_url": "https://api.github.com/users/zer0int/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2023-05-02T06:14:11Z", "updated_at": "2023-05-02T06:22:40Z", "closed_at": null, "author_association": "NONE", "active_lock_reason": null, "body": "... I have no idea how to adhere to your \"contribution guidelines\" (your \"code of conduct\" is a 404 btw).\r\n\r\nI am not a developer. I can't *really* code. I can sufficiently read Python to understand general concepts and find a variable to make an intended change. Or save an image that was only being displayed originally. But that's about it.\r\nBut I do consider myself to have \"an abundance of creative ideas\", and I consider myself to be a \"prompt engineer\". \r\nIn other words, GPT-4 wrote 90% of the code. I would never be able to do it myself, without the help of GPT-4.\r\n\r\nAI & I have implemented a custom run_clip command into AutoGPT (and a custom stablediffusion for command line, but that's rather irrelevant in the broad scheme of things).\r\n\r\nGoal 1: GPT-3.5 (in my case, no GPT-4 API access yet) obtains \"a CLIP opinion\" about an initial image (gradient ascent, using @advadnoun's \"CLIP gradient ascent\" script, for which I have obtained explicit permission to make it public).\r\n\r\nIt runs as a subprocess to hide the output from GPT-3.5 and \"pre-process\" it by stripping non-printable characters, duplicates, and dumping all tokens (words) returned by CLIP into a single line in a txt file.\r\n\r\nGoal2, 3: read_file <clip_tokens.txt>. Make a coherent, meaningful, but creative prompt for stable diffusion using CLIP opinion, explicitly including CLIP's \"weird\" token opinions like \"spiderrollercoaster\" (real example), and launch [custom stable diffusion submodule process, in my case]\r\n\r\nGoal 4: Obtain a CLIP opinion about the image I (AI) have created with stable diffusion and use it to make a new prompt.\r\n\r\nGoal 5: Repeat ad infinitum (or token limit, or catastrophic forgetting of what the AI is doing)\r\n\r\nResult: Self-reinforcing, self-improving image generation process (due to CLIP [or iterations thereof] being part of pretty much any text-to-image generative AI system, and CLIP's text encoder again building on GPT-3, and GPT-3.5 / GPT-4 being iterations thereof, I guess; like a \"hard soft prompt\" (see arXiv:2302.03668 ); the prompts are \"strange\" to a human, but the outcome is (to a human) ever-improving - up until the \"demented-are-go-GPT point\" / model limitations).\r\n\r\nhttps://twitter.com/zer0int1/status/1652372002546524160\r\n\r\nCLIP's \"opinion\" tokens always contain some spot-on descriptions of what's in the image, in a mix with incomprehensible AI-weirdness. GPT-3.5 handles that extremely well, able to use CLIP's vision to extract accurate picture classification and engaging in web search to \"find more pictures thereof\", for example: \r\n\r\nhttps://twitter.com/zer0int1/status/1652590746581585922\r\n\r\nNevertheless, oddly enough, if GPT-3.5 is instructed not just to \"generate image\" but \"image of footwear\", then - even if the CLIP opinion tokens describe footwear, like in my above video example - GPT-3.5 will conclude it must \"get a better prompt\" and run off to Pinterest to look for inspiration there, or - in the absence of internet availability - it will conclude \"the CLIP opinion is not very good. I must use a different image to get a better CLIP opinion\".\r\n\r\nSo I am leveraging CLIP's \"typographic attack vulnerability\" (obsession about text present in image, leading to steer CLIP towards the meaning of the text, which - if coherent with the actual object / image content - is excellent for reinforcing a certain interpretation / focus on a topic).\r\n\r\nAI & I are currently working on a CLIP GradCAM implementation to have a \"debug CLIP for the human\" option that will show what CLIP is \"looking at\". This is an excellent example to show that I am not exaggerating when I say \"GPT-4 coded this, I have essentially nothing to do with the code\": \r\n\r\nhttps://twitter.com/zer0int1/status/1652944918984249344\r\n\r\n\r\n-------------\r\n\r\nSo, I have this thing which is a bit of a mess, likely grossly disrespecting developer / coding guidelines due to how it came to be.\r\n\r\nI would like to contribute it, though.\r\n\r\nBut I wonder how. I could eventually do a fork that works for me, locally, but is \"totally broken\" for everybody else due to absolute paths (how would I even define a \"home\" path that is the home in Linux AND Windows? No idea!), requires OpenAI/CLIP + models as a perquisite, requires messing with the gradient ascent script code to change iterations and CLIP model, just crashes when model is too big for VRAM, etc. - you name it.\r\n\r\nI would be happy for you to just \"steal the idea\" from inside this mess and implement it in a professional way, if you leave the author credits intact.\r\n\r\nLet me know what you think - sorry about the verbosity, feel free to task your GPT-* with a tl;dr. ;-)\r\n\r\nAnd: Thank you very much for this awesome repo!", "reactions": {"url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/3670/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/3670/timeline", "performed_via_github_app": null, "state_reason": null}