{"url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/1360", "repository_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT", "labels_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/1360/labels{/name}", "comments_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/1360/comments", "events_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/1360/events", "html_url": "https://github.com/Significant-Gravitas/Auto-GPT/issues/1360", "id": 1668617073, "node_id": "I_kwDOJKSTjM5jdRNx", "number": 1360, "title": "crashes with error when using GPT-4-32k model in azure.", "user": {"login": "AlphaGeeky23", "id": 128567174, "node_id": "U_kgDOB6nHhg", "avatar_url": "https://avatars.githubusercontent.com/u/128567174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AlphaGeeky23", "html_url": "https://github.com/AlphaGeeky23", "followers_url": "https://api.github.com/users/AlphaGeeky23/followers", "following_url": "https://api.github.com/users/AlphaGeeky23/following{/other_user}", "gists_url": "https://api.github.com/users/AlphaGeeky23/gists{/gist_id}", "starred_url": "https://api.github.com/users/AlphaGeeky23/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AlphaGeeky23/subscriptions", "organizations_url": "https://api.github.com/users/AlphaGeeky23/orgs", "repos_url": "https://api.github.com/users/AlphaGeeky23/repos", "events_url": "https://api.github.com/users/AlphaGeeky23/events{/privacy}", "received_events_url": "https://api.github.com/users/AlphaGeeky23/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 5421676911, "node_id": "LA_kwDOJKSTjM8AAAABQyg5bw", "url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/labels/Azure", "name": "Azure", "color": "047DEA", "default": false, "description": ""}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 25, "created_at": "2023-04-14T17:04:16Z", "updated_at": "2023-04-26T15:28:03Z", "closed_at": null, "author_association": "NONE", "active_lock_reason": null, "body": "### Duplicates\n\n- [X] I have searched the existing issues\n\n### Steps to reproduce \ud83d\udd79\n\nwhile using the GPT-4-32k model (yes, i do have access to it) after entering my 5th \"Goal\" during the initial stage, i get this error message\r\n\n\n### Current behavior \ud83d\ude2f\n\nGoal 5: this the goal text 5 blah...<enter>\r\nWarning: The file 'auto-gpt.json' does not exist. Local memory would not be saved to a file.\r\nUsing memory of type: LocalCache\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Brentf\\source\\repos\\auto-gpt\\scripts\\token_counter.py\", line 17, in count_message_tokens\r\n    encoding = tiktoken.encoding_for_model(model)\r\n  File \"C:\\Users\\Brentf\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tiktoken\\model.py\", line 70, in encoding_for_model\r\n    raise KeyError(\r\nKeyError: 'Could not automatically map GPT-4-32k to a tokeniser. Please use `tiktok.get_encoding` to explicitly get the tokeniser you expect.'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Brentf\\source\\repos\\auto-gpt\\scripts\\main.py\", line 441, in <module>\r\n    main()\r\n  File \"C:\\Users\\Brentf\\source\\repos\\auto-gpt\\scripts\\main.py\", line 304, in main\r\n    agent.start_interaction_loop()\r\n  File \"C:\\Users\\Brentf\\source\\repos\\auto-gpt\\scripts\\main.py\", line 345, in start_interaction_loop\r\n    assistant_reply = chat.chat_with_ai(\r\n  File \"C:\\Users\\Brentf\\source\\repos\\auto-gpt\\scripts\\chat.py\", line 77, in chat_with_ai\r\n    next_message_to_add_index, current_tokens_used, insertion_index, current_context = generate_context(\r\n  File \"C:\\Users\\Brentf\\source\\repos\\auto-gpt\\scripts\\chat.py\", line 40, in generate_context\r\n    current_tokens_used = token_counter.count_message_tokens(current_context, model)\r\n  File \"C:\\Users\\Brentf\\source\\repos\\auto-gpt\\scripts\\token_counter.py\", line 19, in count_message_tokens\r\n    logger.warn(\"Warning: model not found. Using cl100k_base encoding.\")\r\nNameError: name 'logger' is not defined\n\n### Expected behavior \ud83e\udd14\n\nexpected = no error\n\n### Your prompt \ud83d\udcdd\n\n```yaml\r\n# Paste your prompt here\r\n```\r\n", "reactions": {"url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/1360/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/Significant-Gravitas/Auto-GPT/issues/1360/timeline", "performed_via_github_app": null, "state_reason": null}