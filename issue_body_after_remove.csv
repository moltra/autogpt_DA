,index,number,body
0,0,785,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

Right now Auto-GPT does not provide any feedback on the goals and role etc.

To make sure it understood the assignment, it would be much better for it to reinterpret the prompt.

That way it's more of a dialogue than just an assignment that get's never updated.

Also adding another prompt command ""reevaluate_assignment"" or something would be great then.

### Examples üåà

I already instructed Auto-GPT to build such a script,
it didn't finish and started doing that weird gpt-2 clone and tryint to execute gpt-2 training.

But this script kindda made sense:

``` analyze_code.py
import yaml
from typing import List

def analyze_code(code: str) -> List[str]:
    try:
        with open('ai_settings.yaml', 'r') as file:
            ai_settings = yaml.safe_load(file)
            current_goals_dict = ai_settings['ai_goals']
            current_goals = list(current_goals_dict.values())
    except FileNotFoundError:
        return ['Error: ai_settings.yaml file not found.']
    except yaml.YAMLError:
        return ['Error: ai_settings.yaml file is not valid YAML.']
    
    try:
        with open('new_goals.txt', 'r') as file:
            new_goals = [goal.strip() for goal in file.readlines()]
    except FileNotFoundError:
        return ['Error: new_goals.txt file not found.']
    
    revised_goals = list(set(current_goals) | set(new_goals))
    
    revised_goals_dict = {'goal'+str(i): goal for i, goal in enumerate(revised_goals)}
    ai_settings['ai_goals'] = revised_goals_dict
    
    try:
        with open('ai_settings.yaml', 'w') as file:
            yaml.dump(ai_settings, file)
            return ['Updated goals:'] + revised_goals
    except:
        return ['Error: failed to write to ai_settings.yaml file.']
```

### Motivation üî¶

_No response_"
1,1,778,"### Duplicates

- [X] I have searched the existing issues

### Steps to reproduce üïπ

1. Run Auto-GPT as Docker image via VS Code.
2. Provide goals for Auto-GPT.
3. Write ""y"" command few times.
4. Write ""y -10"" command few times.
5. Suddenly Auto-GPT fails after another ""y -10"" command
6. Auto-GPT terminated

### Current behavior üòØ

```txt
NEXT ACTION:  COMMAND = start_agent ARGUMENTS = {'name': 'DiplomacyChallengeSolver', 'task': 'placeholder'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for Peacemaker-AI...
Input:y -10

**Traceback (most recent call last):
  File ""/app/main.py"", line 406, in <module>
    memory.add(memory_to_add)
  File ""/app/memory/local.py"", line 64, in add
    out = orjson.dumps(
          ^^^^^^^^^^^^^
TypeError: str is not valid UTF-8: surrogates not allowed**
```

### Expected behavior ü§î

1. No exception after ""y -10"" command
2. Workaround for this type of exception to prevent Auto-GPT from termination

### Your prompt üìù

```yaml
# Paste your prompt here
```"
2,2,766,"<!-- üì¢ Announcement
We've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to maintain a clean and manageable git history. To ensure the quality of our repository, we kindly ask you to adhere to the following guidelines when submitting PRs:

Focus on a single, specific change.
Do not include any unrelated or ""extra"" modifications.
Provide clear documentation and explanations of the changes made.
Ensure diffs are limited to the intended lines ‚Äî no applying preferred formatting styles or line endings (unless that's what the PR is about).
For guidance on committing only the specific lines you have changed, refer to this helpful video: https://youtu.be/8-hSNHHbiZg

By following these guidelines, your PRs are more likely to be merged quickly after testing, as long as they align with the project's overall direction. -->

### Background
<!-- Provide a concise overview of the rationale behind this change. Include relevant context, prior discussions, or links to related issues. Ensure that the change aligns with the project's overall direction. -->

A user can communicate with Auto-GPT in a language other than English that does not contain the letters y, n. In this case, the user have to change the keyboard layout between simple and text response to English and back. The numbers are (most likely) the same in any layout.

### Changes
<!-- Describe the specific, focused change made in this pull request. Detail the modifications clearly and avoid any unrelated or ""extra"" changes. -->

The addition of 1/0 has been added as alternatives to y/n in the main.py script to allow non-English speaking users to communicate with Auto-GPT more easily.

### Documentation
<!-- Explain how your changes are documented, such as in-code comments or external documentation. Ensure that the documentation is clear, concise, and easy to understand. -->

In the main.py script, a comment has been added to explain that 1/0 can be used as an alternative to y/n for non-English speaking users.

### Test Plan
<!-- Describe how you tested this functionality. Include steps to reproduce, relevant test cases, and any other pertinent information. -->

To test this functionality, I performed the following steps:
- Changed the keyboard layout to a language other than English that does not contain the letters y, n.
- Prompted Auto-GPT for a response that requires a y/n input.
- Verified that entering 1/0 instead of y/n returned the expected response from Auto-GPT.

As this is a relatively small and straightforward change, no new test cases were added.

### PR Quality Checklist
- [x] My pull request is atomic and focuses on a single change.
- [x] I have thouroughly tested my changes with multiple different prompts.
- [x] I have considered potential risks and mitigations for my changes.
- [x] I have documented my changes clearly and comprehensively.
- [x] I have not snuck in any ""extra"" small tweaks changes <!-- Submit these as seperate Pull Reqests, they are the easiest to merge! -->

<!-- If you haven't added tests, please explain why. If you have, check the appropriate box. If you've ensured your PR is atomic and well-documented, check the corresponding boxes. -->

<!-- By submitting this, I agree that my pull request should be closed if I do not fill this out or follow the guide lines. -->
"
3,3,765,"<!-- üì¢ Announcement
We've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to maintain a clean and manageable git history. To ensure the quality of our repository, we kindly ask you to adhere to the following guidelines when submitting PRs:

Focus on a single, specific change.
Do not include any unrelated or ""extra"" modifications.
Provide clear documentation and explanations of the changes made.
Ensure diffs are limited to the intended lines ‚Äî no applying preferred formatting styles or line endings (unless that's what the PR is about).
For guidance on committing only the specific lines you have changed, refer to this helpful video: https://youtu.be/8-hSNHHbiZg

By following these guidelines, your PRs are more likely to be merged quickly after testing, as long as they align with the project's overall direction. -->

### Background
The goal is to build an AGI. This AGI might be one single Auto-GPT that loops against itself, but I doubt it because the LLM is super slow. We can't expect to have something that has high computing power with only one autonomous agent. And we can't have one agent per computing machine, that's stupid, because most of the time the running process will just hang and do nothing, waiting for Open AI to give back an answer. Waste of resources. Running multiple agents in different tabs is not ideal either because processes are heavier than threads.
On the current PR I was able to spawn 20 concurrent workers simultaneously without any problem.

So I think that in order to build an AGI, we need to build an entity able to:
- Define autonomous agents.
- Start autonomous agents
- Monitor autonomous agents. This means being able to identify all the meaningful events happening to them and talk to them. (this is not part of this PR unfortunately, it's too many changes at once, but I have a branch ready for it) => 
- Stop these workers when necessary.

Which programmatic language is the AGI going to use to do that? A proper, well-defined REST API is the way to go, we can only do so much with a CLI.

This AGI will talk to its workers through rest api calls, which are nothing more than JSON sent over the network.

At the beginning we will be playing the AGI, so we will use these endpoints to communicate with our agents, through a web browser UI.

### Changes
Now let's come back to reality, I had to work with what the repo provides, so I made the smallest changes possible to achieve these goals. **The first thing I want to say, is that my changes won't affect the users in any way. All the existing features are preserved and unchanged.**

Additions to the repo **that are all necessary to achieve this User Story** (I insist they're all necessary to be merged together):
- A well-structured web server with FastAPI.
- Solid integration tests, extremely readable because all the expected requests and responses are written. These integration tests record the behavior of the autonomous agents. I will use these tests to maintain an automatically generated documentation of auto-gpt api. 
- The integration tests work with GPT3.5 and the cost is very small. Talking about cost, @Torantulino please create an online bank account for auto-gpt organization so we can wire money and pay for an api key that will use to call open AI within github action. You can keep the key private. I will wire 100$ to the bank account when you have it ready. Also, I know some companies interested sponsoring auto-gpt, so reach out to me on discord.
- Ability to stop an autonomous agent gracefully, without forcing it. Each agent runs in separates threads.
- a NoMemory class. Why is that ? Because right now memories are shared accross the same worker !!! So it means if we spawn multiple autonomous agents, they will step into each other's toes. The great news is that NoMemory is actually super performant, I have tested it and it performs oftentimes better than pinecone, mostly because we load an ugly JSON of all the past memories and it confuses the AI. Particularly with GPT 3.5. It turns out summarizing the content as we go and loosing any non essential information can be great for short tasks.
- the server is currently in a folder called app with the right project structure for a project of this size. This was the easiest way to do it without breaking the whole repo. This repo has no module, and I can't put the api under a folder called ""scripts"", that's beyond me :)

CHANGES to the repo:
- finally wrap the main.py into an ""if ___main___"", otherwise nothing in this project is reusable, right now I can't import the main.py without starting the loop haha
- create the autonomous agent class so that it can start the start_loop method
- create attributes of the class. It's pretty much adding self everywhere.
- The temperature is currently None in the repo which means 1. The values are between 0 and 2. For my tests I need a very consistent LLM, so I need 0. So I just put an environment variable. The value will still be 1 for others and nothing changes. (I recommend putting the default value to 0 by the way this is a game changer, but that's another story...)

### Documentation
Look at the tests, they document clearly the following requests and responses as well as the output of the worker:
2 integration tests at the moment: 
- Autonomous agent defined (POST), started (PATCH) STOPPED immediately after starting (PATCH)
- Autonomous agent defined (POST), started (PATCH), Writes a file and shuts down on its own. When we try to shut it down we get a 500 because it's already down.
![Screenshot 2023-04-10 at 8 32 34 PM](https://user-images.githubusercontent.com/9652976/231049456-95d0950e-8996-4766-a61a-1fe0f5cb9999.png)
![Screenshot 2023-04-10 at 8 32 29 PM](https://user-images.githubusercontent.com/9652976/231049458-55217c4c-ead9-4e15-a5fb-89a83cb428f5.png)
![Screenshot 2023-04-10 at 8 32 24 PM](https://user-images.githubusercontent.com/9652976/231049459-ba1fc769-2b78-49e9-99e6-1bcf25dba8c4.png)
![Screenshot 2023-04-10 at 8 32 16 PM](https://user-images.githubusercontent.com/9652976/231049461-ca10c4ff-95a2-41d8-9cd6-223b26c94e33.png)




### Test Plan
The CLI has to keep working !!! So I tested it thoroughly Here is one run example.
<img width=""1492"" alt=""Screenshot 2023-04-10 at 8 27 02 PM"" src=""https://user-images.githubusercontent.com/9652976/231049559-304ab597-704f-44a1-8715-4d5ef560052c.png"">
<img width=""1491"" alt=""Screenshot 2023-04-10 at 8 27 12 PM"" src=""https://user-images.githubusercontent.com/9652976/231049569-332316fe-07a8-4c95-99db-c9fb9de46541.png"">
### PR Quality Checklist
- [X] My pull request is atomic and focuses on a single change.
- [X] I have thouroughly tested my changes with multiple different prompts.
- [X] I have considered potential risks and mitigations for my changes.
- [X] I have documented my changes clearly and comprehensively.
- [X] I have not snuck in any ""extra"" small tweaks changes <!-- Submit these as seperate Pull Reqests, they are the easiest to merge! -->

<!-- If you haven't added tests, please explain why. If you have, check the appropriate box. If you've ensured your PR is atomic and well-documented, check the corresponding boxes. -->

<!-- By submitting this, I agree that my pull request should be closed if I do not fill this out or follow the guide lines. -->
"
4,4,758,"### Background
This change was made to the `ai_config.py` file #185. However, based on a suggestion, the knowledge cutoff was directly added to the `prompt.txt` file. This aims to provide metadata to the AI, enhancing its search coordination and thought processes.

### Changes
A single line was added on top of `prompt.txt` file. The line added is `Knowledge Cutoff Date: Please remember that your knowledge and information are accurate and up-to-date as of September 2021. Please use this information to better coordinate your searches and thoughts.`

### Documentation
Unfortunately, no documentation is made.

### Test Plan
No test was conducted.

### PR Quality Checklist
- [x] My pull request is atomic and focuses on a single change.
- [ ] I have thoroughly tested my changes with multiple different prompts.
- [x] I have considered potential risks and mitigations for my changes.
- [x] I have documented my changes clearly and comprehensively.
- [x] I have not included any ""extra"" small tweaks or changes."
5,5,753,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

Add `1/0` as alternative to `y/n`

### Examples üåà

So in
`Continue (y/n)` and `Enter \'y\' to authorise command, \'y -N\' to run N continuous commands, \'n\' to exit program`
we can use
`Continue (1/0)` and `Enter \'1\' to authorise command, \'1 -N\' to run N continuous commands, \'0\' to exit program`

### Motivation üî¶

A user can communicate with Auto-GPT in a language other than English that does not contain the letters `y`, `n`. In this case, the user have to change the keyboard layout between simple and text response to English and back. The numbers are (most likely) the same in any layout."
6,6,749,"Bitcoin handling for payments via bitcoin_utils.py

<!-- üì¢ Announcement
We've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to maintain a clean and manageable git history. To ensure the quality of our repository, we kindly ask you to adhere to the following guidelines when submitting PRs:

Focus on a single, specific change.
Do not include any unrelated or ""extra"" modifications.
Provide clear documentation and explanations of the changes made.
Ensure diffs are limited to the intended lines ‚Äî no applying preferred formatting styles or line endings (unless that's what the PR is about).
For guidance on committing only the specific lines you have changed, refer to this helpful video: https://youtu.be/8-hSNHHbiZg

By following these guidelines, your PRs are more likely to be merged quickly after testing, as long as they align with the project's overall direction. -->

### Background
<!-- Provide a concise overview of the rationale behind this change. Include relevant context, prior discussions, or links to related issues. Ensure that the change aligns with the project's overall direction. -->

### Changes
<!-- Describe the specific, focused change made in this pull request. Detail the modifications clearly and avoid any unrelated or ""extra"" changes. -->

### Documentation
<!-- Explain how your changes are documented, such as in-code comments or external documentation. Ensure that the documentation is clear, concise, and easy to understand. -->

### Test Plan
<!-- Describe how you tested this functionality. Include steps to reproduce, relevant test cases, and any other pertinent information. -->

### PR Quality Checklist
- [ ] My pull request is atomic and focuses on a single change.
- [ ] I have thouroughly tested my changes with multiple different prompts.
- [ ] I have considered potential risks and mitigations for my changes.
- [ ] I have documented my changes clearly and comprehensively.
- [ ] I have not snuck in any ""extra"" small tweaks changes <!-- Submit these as seperate Pull Reqests, they are the easiest to merge! -->

<!-- If you haven't added tests, please explain why. If you have, check the appropriate box. If you've ensured your PR is atomic and well-documented, check the corresponding boxes. -->

<!-- By submitting this, I agree that my pull request should be closed if I do not fill this out or follow the guide lines. -->
"
7,7,727,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

Currently Auto-GPT writes the entire python or text file at once. If the file gets beyond a few paragraphs this starts to cause issues often time causing json parsing error. Could we somehow have it insert text into an existing file. Maybe we can have it give vim commands to edit it.

Been running a benchmark of having it write a fluid simulation using the lattice boltzmann method. Been running this everyday for a week or so to see the progress of Auto-GPT developments. I think it is very close to working but need this one last bit of functionality.

### Examples üåà

Here is an example that I think shows gpt is capable of doing it.
![Screenshot from 2023-04-10 12-03-14](https://user-images.githubusercontent.com/4978408/230974929-033d978a-7ede-4bd4-b8dc-d6709afccfb8.png)

![Screenshot from 2023-04-10 12-02-52](https://user-images.githubusercontent.com/4978408/230974919-dad15015-8279-42a5-99d9-5d3094edc19f.png)



### Motivation üî¶

This would allow for Auto-GPT to modify and improve itself "
8,8,718,"**EDIT: Please delete this PR as #697 seems to be a better solution**

Fixes #701 Credits belong to @Moomero

### Background
INVALID JSON errors happen very commonly. 

### Changes
This fix filters better and reduces the amount of such errors.

### PR Quality Checklist
- [x] My pull request is atomic and focuses on a single change.
- [x] I have thoroughly tested my changes with multiple different prompts.
- [x] I have considered potential risks and mitigations for my changes.
- [ ] I have documented my changes clearly and comprehensively.
- [x] I have not snuck in any ""extra"" small tweaks changes"
9,9,691,"Add additional clarification to error message as this is the first check of the model name.  An alternative approach would be to add a verification of the model before this is called.

<!-- üì¢ Announcement
We've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to maintain a clean and manageable git history. To ensure the quality of our repository, we kindly ask you to adhere to the following guidelines when submitting PRs:

Focus on a single, specific change.
Do not include any unrelated or ""extra"" modifications.
Provide clear documentation and explanations of the changes made.
Ensure diffs are limited to the intended lines ‚Äî no applying preferred formatting styles or line endings (unless that's what the PR is about).
For guidance on committing only the specific lines you have changed, refer to this helpful video: https://youtu.be/8-hSNHHbiZg

By following these guidelines, your PRs are more likely to be merged quickly after testing, as long as they align with the project's overall direction. -->

### Background
<!-- Provide a concise overview of the rationale behind this change. Include relevant context, prior discussions, or links to related issues. Ensure that the change aligns with the project's overall direction. -->

### Changes
<!-- Describe the specific, focused change made in this pull request. Detail the modifications clearly and avoid any unrelated or ""extra"" changes. -->

### Documentation
<!-- Explain how your changes are documented, such as in-code comments or external documentation. Ensure that the documentation is clear, concise, and easy to understand. -->

### Test Plan
<!-- Describe how you tested this functionality. Include steps to reproduce, relevant test cases, and any other pertinent information. -->

### PR Quality Checklist
- [ ] My pull request is atomic and focuses on a single change.
- [ ] I have thouroughly tested my changes with multiple different prompts.
- [ ] I have considered potential risks and mitigations for my changes.
- [ ] I have documented my changes clearly and comprehensively.
- [ ] I have not snuck in any ""extra"" small tweaks changes <!-- Submit these as seperate Pull Reqests, they are the easiest to merge! -->

<!-- If you haven't added tests, please explain why. If you have, check the appropriate box. If you've ensured your PR is atomic and well-documented, check the corresponding boxes. -->

<!-- By submitting this, I agree that my pull request should be closed if I do not fill this out or follow the guide lines. -->
"
10,10,660,"https://github.com/Torantulino/Auto-GPT/issues/656#issue-1660550311

Fix from updated line 20 from \r to \n

<!-- üì¢ Announcement
We've recently noticed an increase in pull requests focusing on combining multiple changes. While the intentions behind these PRs are appreciated, it's essential to maintain a clean and manageable git history. To ensure the quality of our repository, we kindly ask you to adhere to the following guidelines when submitting PRs:

Focus on a single, specific change.
Do not include any unrelated or ""extra"" modifications.
Provide clear documentation and explanations of the changes made.
Ensure diffs are limited to the intended lines ‚Äî no applying preferred formatting styles or line endings (unless that's what the PR is about).
For guidance on committing only the specific lines you have changed, refer to this helpful video: https://youtu.be/8-hSNHHbiZg

By following these guidelines, your PRs are more likely to be merged quickly after testing, as long as they align with the project's overall direction. -->

### Background
<!-- Provide a concise overview of the rationale behind this change. Include relevant context, prior discussions, or links to related issues. Ensure that the change aligns with the project's overall direction. -->

### Changes
<!-- Describe the specific, focused change made in this pull request. Detail the modifications clearly and avoid any unrelated or ""extra"" changes. -->

### Documentation
<!-- Explain how your changes are documented, such as in-code comments or external documentation. Ensure that the documentation is clear, concise, and easy to understand. -->

### Test Plan
<!-- Describe how you tested this functionality. Include steps to reproduce, relevant test cases, and any other pertinent information. -->

### PR Quality Checklist
- [x] My pull request is atomic and focuses on a single change.
- [x] I have thouroughly tested my changes with multiple different prompts.
- [x] I have considered potential risks and mitigations for my changes.
- [x] I have documented my changes clearly and comprehensively.
- [x] I have not snuck in any ""extra"" small tweaks changes <!-- Submit these as seperate Pull Reqests, they are the easiest to merge! -->

<!-- If you haven't added tests, please explain why. If you have, check the appropriate box. If you've ensured your PR is atomic and well-documented, check the corresponding boxes. -->

<!-- By submitting this, I agree that my pull request should be closed if I do not fill this out or follow the guide lines. -->
"
11,11,623,"### Duplicates

- [X] I have searched the existing issues

### Steps to reproduce üïπ

1. Set API Keys for OpenAI, Pinecone, and Eleven Labs, then set the smart model to gpt-3.5-turbo
2. Start AutoGPT with the --gpt3only option
3. Prompt GPT to be a developer, and give it the goal to load a design document text file into long term memory. and the additional goals to develop the project.

### Current behavior üòØ

Currently the AI will read_file the document, and will output a JSON format error as the document is long, hence the need to load it into long term memory, and it just replies with ""sure, I got that""
`````````
F:\AUTOGPT>call python ./scripts/main.py --gpt3only --debug
GPT3.5 Only Mode:  ENABLED
Debug Mode:  ENABLED
Welcome back!  Would you like me to return to being Unity-Dev-GPT?
Continue with the last settings?
Name:  Unity-Dev-GPT
Role:  An AI designed to build Video Games, Libraries, Editor Extensions, and Packages for Unity LTS 2021.
Goals: ['Load the design document file under `./designdocument.txt` into long term memory.', 'Find out the currently created files in the unity project under `./UnityProject/Assets`, then improve the code according to the design document, solving any issues or bugs found.', 'Find out what needs to be implemented next, then implement it, then do this step again until there is nothing left to implement.', 'Double check and make sure everything is implemented in the Unity Project Files from your understanding of the project, then refine and criticize the code making sure everything is in working order.', 'Shut Down.']
Continue (y/n): y
Using memory of type: LocalCache
Token limit: 4000
Memory Stats:  (0, (0, 1536))
Token limit: 4000
Send Token Count: 972
Tokens remaining for response: 3028
------------ CONTEXT SENT TO AI ---------------
System: The current time and date is Sun Apr  9 14:13:43 2023

System: This reminds you of these events from your past:
[]



User: Determine which next command to use, and respond using the format specified above:

----------- END OF CONTEXT ----------------
UNITY-DEV-GPT THOUGHTS: My next step should be to load the design document into long-term memory and review it to understand the current status of the Unity Project. I will use the 'read_file' command to read the file and save the content to long-term memory.
REASONING: I need to get an understanding of the current state of the Unity Project to determine what needs to be implemented next.
PLAN:
-  Read the design document using the 'read_file' command
-  Save the content to long-term memory
-  Review the content to determine the current state of the Unity Project
CRITICISM: The plan is straightforward and efficient, but I should always be careful not to miss or misunderstand any information from the document.
NEXT ACTION:  COMMAND = read_file ARGUMENTS = {'file': './designdocument.txt'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for Unity-Dev-GPT...
Input:y
-=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=
SYSTEM:  Command read_file returned: ...Long Design Document...
Token limit: 4000
Memory Stats:  (1, (1, 1536))
Token limit: 4000
Send Token Count: 961
Tokens remaining for response: 3039
------------ CONTEXT SENT TO AI ---------------
System: The current time and date is Sun Apr  9 14:13:55 2023

System: This reminds you of these events from your past:
[]



User: GENERATE NEXT COMMAND JSON

----------- END OF CONTEXT ----------------
Warning: Failed to parse AI output, attempting to fix.
 If you see this warning frequently, it's likely that your prompt is confusing the AI. Try changing it up slightly.
------------ JSON FIX ATTEMPT ---------------
Original JSON: ```json
Understood. What would you like me to do next?
```
-----------
Fixed JSON: I'm sorry, but I cannot provide a valid `return` value without knowing the schema that the JSON string should comply with. Please provide the schema or more information about the expected format of the JSON.
----------- END OF FIX ATTEMPT ----------------
Failed to fix ai output, telling the AI.
Error: Invalid JSON
 Understood. What would you like me to do next?
UNITY-DEV-GPT THOUGHTS:
REASONING:
CRITICISM:
Warning: Failed to parse AI output, attempting to fix.
 If you see this warning frequently, it's likely that your prompt is confusing the AI. Try changing it up slightly.
------------ JSON FIX ATTEMPT ---------------
Original JSON: ```json
Understood. What would you like me to do next?
```
-----------
Fixed JSON: I'm sorry, but I cannot provide a valid `return` value without knowing the schema that the JSON string should comply with. Please provide the schema or more information about the expected format of the JSON.
----------- END OF FIX ATTEMPT ----------------
Failed to fix ai output, telling the AI.
NEXT ACTION:  COMMAND = Error: ARGUMENTS = Missing 'command' object in JSON
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for Unity-Dev-GPT...
Input:
`````````

I forgot to setup the docker redis, and after I did the error was the same, here is the console output with uneeded data removed:
``````````
F:\AUTOGPT>call python ./scripts/main.py --gpt3only --debug
GPT3.5 Only Mode:  ENABLED
Debug Mode:  ENABLED
Welcome back!  Would you like me to return to being Unity-Dev-GPT?
Continue with the last settings?
Name:  Unity-Dev-GPT
Role:  An AI designed to build Video Games, Libraries, Editor Extensions, and Packages for Unity LTS 2021.
Goals: ['Load the design document file under `./designdocument.txt` into long term memory.', 'Find out the currently created files in the unity project under `./UnityProject/Assets`, then improve the code according to the design document, solving any issues or bugs found.', 'Find out what needs to be implemented next, then implement it, then do this step again until there is nothing left to implement.', 'Double check and make sure everything is implemented in the Unity Project Files from your understanding of the project, then refine and criticize the code making sure everything is in working order.', 'Shut Down.']
Continue (y/n): y
Using memory of type: RedisMemory
Token limit: 4000
Memory Stats:  {'index_name': 'mypineconeindex', 'index_options': [], 'index_definition': [b'key_type', b'HASH', b'prefixes', [b'mypineconeindex:'], b'default_score', b'1'], 'attributes': [[b'identifier', b'data', b'attribute', b'data', b'type', b'TEXT', b'WEIGHT', b'1'], [b'identifier', b'embedding', b'attribute', b'embedding', b'type', b'VECTOR']], 'num_docs': '0', 'max_doc_id': '0', 'num_terms': '0', 'num_records': '0', 'inverted_sz_mb': '0', 'vector_index_sz_mb': '0', 'total_inverted_index_blocks': '0', 'offset_vectors_sz_mb': '0', 'doc_table_size_mb': '0', 'sortable_values_size_mb': '0', 'key_table_size_mb': '0', 'records_per_doc_avg': '-nan', 'bytes_per_record_avg': '-nan', 'offsets_per_term_avg': '-nan', 'offset_bits_per_record_avg': '-nan', 'hash_indexing_failures': '0', 'total_indexing_time': '0', 'indexing': '0', 'percent_indexed': '1', 'number_of_uses': 2, 'gc_stats': [b'bytes_collected', b'0', b'total_ms_run', b'0', b'total_cycles', b'0', b'average_cycle_time_ms', b'-nan', b'last_run_time_ms', b'0', b'gc_numeric_trees_missed', b'0', b'gc_blocks_denied', b'0'], 'cursor_stats': [b'global_idle', 0, b'global_total', 0, b'index_capacity', 128, b'index_total', 0], 'dialect_stats': [b'dialect_1', 0, b'dialect_2', 1, b'dialect_3', 0]}
Token limit: 4000
Send Token Count: 972
Tokens remaining for response: 3028
------------ CONTEXT SENT TO AI ---------------
System: The current time and date is Sun Apr  9 14:06:23 2023

System: This reminds you of these events from your past:
[]



User: Determine which next command to use, and respond using the format specified above:

----------- END OF CONTEXT ----------------
UNITY-DEV-GPT THOUGHTS: Let's start by loading the design document file into long term memory.
REASONING: Before we can proceed to improve the Unity project, we need to have a full understanding of what the project design entails. Loading the design document into long term memory will allow us to draw upon this information anytime we need it.
PLAN:
-  Load the design document file under `./designdocument.txt` into long term memory.
CRITICISM: I can improve on this by breaking down the design document into smaller, more manageable chunks that will be easier for me to load into long term memory.
NEXT ACTION:  COMMAND = read_file ARGUMENTS = {'file': './designdocument.txt'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for Unity-Dev-GPT...
Input:y
-=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=
SYSTEM:  Command read_file returned: LONG DESIGN DOCUMENT TEXT HERE
Token limit: 4000
Memory Stats:  {'index_name': 'IsetTheIndexToMyPineconeIndex', 'index_options': [], 'index_definition': [b'key_type', b'HASH', b'prefixes', [b'IsetTheIndexToMyPineconeIndex:'], b'default_score', b'1'], 'attributes': [[b'identifier', b'data', b'attribute', b'data', b'type', b'TEXT', b'WEIGHT', b'1'], [b'identifier', b'embedding', b'attribute', b'embedding', b'type', b'VECTOR']], 'num_docs': '1', 'max_doc_id': '1', 'num_terms': '1325', 'num_records': '1326', 'inverted_sz_mb': '0.011920928955078125', 'vector_index_sz_mb': '6.1705780029296875', 'total_inverted_index_blocks': '1325', 'offset_vectors_sz_mb': '0.0056028366088867188', 'doc_table_size_mb': '0.00012302398681640625', 'sortable_values_size_mb': '0', 'key_table_size_mb': '2.765655517578125e-05', 'records_per_doc_avg': '1326', 'bytes_per_record_avg': '9.4268474578857422', 'offsets_per_term_avg': '2.8122172355651855', 'offset_bits_per_record_avg': '12.603915214538574', 'hash_indexing_failures': '0', 'total_indexing_time': '1.742', 'indexing': '0', 'percent_indexed': '1', 'number_of_uses': 4, 'gc_stats': [b'bytes_collected', b'0', b'total_ms_run', b'0', b'total_cycles', b'0', b'average_cycle_time_ms', b'-nan', b'last_run_time_ms', b'0', b'gc_numeric_trees_missed', b'0', b'gc_blocks_denied', b'0'], 'cursor_stats': [b'global_idle', 0, b'global_total', 0, b'index_capacity', 128, b'index_total', 0], 'dialect_stats': [b'dialect_1', 0, b'dialect_2', 1, b'dialect_3', 0]}
Token limit: 4000
Send Token Count: 961
Tokens remaining for response: 3039
------------ CONTEXT SENT TO AI ---------------
System: The current time and date is Sun Apr  9 14:06:56 2023

System: This reminds you of these events from your past:
[]



User: GENERATE NEXT COMMAND JSON

----------- END OF CONTEXT ----------------
Warning: Failed to parse AI output, attempting to fix.
 If you see this warning frequently, it's likely that your prompt is confusing the AI. Try changing it up slightly.
------------ JSON FIX ATTEMPT ---------------
Original JSON: ```json
Sure thing! What task would you like me to perform next?
```
-----------
Fixed JSON: I'm sorry, I don't see a task specified in your message. Please let me know what task you would like me to perform.
----------- END OF FIX ATTEMPT ----------------
Failed to fix ai output, telling the AI.
Error: Invalid JSON
 Sure thing! What task would you like me to perform next?
UNITY-DEV-GPT THOUGHTS:
REASONING:
CRITICISM:
Warning: Failed to parse AI output, attempting to fix.
 If you see this warning frequently, it's likely that your prompt is confusing the AI. Try changing it up slightly.
------------ JSON FIX ATTEMPT ---------------
Original JSON: ```json
Sure thing! What task would you like me to perform next?
```
-----------
Fixed JSON: I'm sorry, I don't see a task specified in your message. Please let me know what task you would like me to perform.
----------- END OF FIX ATTEMPT ----------------
Failed to fix ai output, telling the AI.
NEXT ACTION:  COMMAND = Error: ARGUMENTS = Missing 'command' object in JSON
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for Unity-Dev-GPT...
Input:What is the title of the design document?
SYSTEM:  Human feedback: What is the title of the design document?
Token limit: 4000
Memory Stats:  {'index_name': 'IsetTheIndexToMyPineconeIndex', 'index_options': [], 'index_definition': [b'key_type', b'HASH', b'prefixes', [b'IsetTheIndexToMyPineconeIndex:'], b'default_score', b'1'], 'attributes': [[b'identifier', b'data', b'attribute', b'data', b'type', b'TEXT', b'WEIGHT', b'1'], [b'identifier', b'embedding', b'attribute', b'embedding', b'type', b'VECTOR']], 'num_docs': '2', 'max_doc_id': '2', 'num_terms': '1330', 'num_records': '1348', 'inverted_sz_mb': '0.012048721313476562', 'vector_index_sz_mb': '6.1706771850585938', 'total_inverted_index_blocks': '1330', 'offset_vectors_sz_mb': '0.0056304931640625', 'doc_table_size_mb': '0.0002460479736328125', 'sortable_values_size_mb': '0', 'key_table_size_mb': '8.296966552734375e-05', 'records_per_doc_avg': '674', 'bytes_per_record_avg': '9.3724031448364258', 'offsets_per_term_avg': '2.7878339290618896', 'offset_bits_per_record_avg': '12.568387031555176', 'hash_indexing_failures': '0', 'total_indexing_time': '1.8500000000000001', 'indexing': '0', 'percent_indexed': '1', 'number_of_uses': 6, 'gc_stats': [b'bytes_collected', b'0', b'total_ms_run', b'0', b'total_cycles', b'0', b'average_cycle_time_ms', b'-nan', b'last_run_time_ms', b'0', b'gc_numeric_trees_missed', b'0', b'gc_blocks_denied', b'0'], 'cursor_stats': [b'global_idle', 0, b'global_total', 0, b'index_capacity', 128, b'index_total', 0], 'dialect_stats': [b'dialect_1', 0, b'dialect_2', 1, b'dialect_3', 0]}
Token limit: 4000
Send Token Count: 1066
Tokens remaining for response: 2934
------------ CONTEXT SENT TO AI ---------------
System: The current time and date is Sun Apr  9 14:07:16 2023

System: This reminds you of these events from your past:
['Assistant Reply: Sure thing! What task would you like me to perform next? \nResult: Human feedback: What is the title of the design document? \nHuman Feedback: What is the title of the design document? ']



User: GENERATE NEXT COMMAND JSON

Assistant: Sure thing! What task would you like me to perform next?

System: Human feedback: What is the title of the design document?

User: What is the title of the design document?

----------- END OF CONTEXT ----------------
Warning: Failed to parse AI output, attempting to fix.
 If you see this warning frequently, it's likely that your prompt is confusing the AI. Try changing it up slightly.
------------ JSON FIX ATTEMPT ---------------
Original JSON: ```json
I'm sorry but I don't have this information in my memory. Would you be able to provide me with the title of the design document so that I can load it into my long term memory?
```
-----------
Fixed JSON: I apologize, but the provided code block appears to be a sample JSON object and not the design document for the `fix_json` function. Could you please provide me with the design document for the function?
----------- END OF FIX ATTEMPT ----------------
Failed to fix ai output, telling the AI.
Error: Invalid JSON
 I'm sorry but I don't have this information in my memory. Would you be able to provide me with the title of the design document so that I can load it into my long term memory?
UNITY-DEV-GPT THOUGHTS:
REASONING:
CRITICISM:
Warning: Failed to parse AI output, attempting to fix.
 If you see this warning frequently, it's likely that your prompt is confusing the AI. Try changing it up slightly.
------------ JSON FIX ATTEMPT ---------------
Original JSON: ```json
I'm sorry but I don't have this information in my memory. Would you be able to provide me with the title of the design document so that I can load it into my long term memory?
```
-----------
Fixed JSON: I apologize, but the provided code block appears to be a JSON object and not the title of a design document. Please provide me with the correct information so that I can assist you better.
----------- END OF FIX ATTEMPT ----------------
Failed to fix ai output, telling the AI.
NEXT ACTION:  COMMAND = Error: ARGUMENTS = Missing 'command' object in JSON
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for Unity-Dev-GPT...
Input:
``````````

### Expected behavior ü§î

The bot should not read the file but just send it directly to memory as to not overflow the token limit, or if it should be able to do it even with large token files why is it not working?

### Your prompt üìù

```yaml
ai_goals:
- Load the design document file under `./designdocument.txt` into long term memory.
- Find out the currently created files in the unity project under `./UnityProject/Assets`,
  then improve the code according to the design document, solving any issues or bugs
  found.
- Find out what needs to be implemented next, then implement it, then do this step
  again until there is nothing left to implement.
- Double check and make sure everything is implemented in the Unity Project Files
  from your understanding of the project, then refine and criticize the code making
  sure everything is in working order.
- Shut Down.
ai_name: Unity-Dev-GPT
ai_role: An AI designed to build Video Games, Libraries, Editor Extensions, and Packages
  for Unity LTS 2021.
```"
12,12,605,"### Background

<!-- Provide a brief overview of why this change is being made. Include any relevant context, prior discussions, or links to relevant issues. -->

The aim of this PR is to make the use of Redis Stack beginner friendly and help developers get consistent results.
Developers and curious people can access RedisInsight at  http://localhost:8001/ to see the interaction between Auto-GPT and Redis caching engine.

### Changes

<!-- Describe the changes made in this pull request. Be specific and detailed. -->

Made it easier to use Redis by providing the steps in the updated documentation.
Consistent build, configuration and testing using a docker-compose.yml type of configuration.
Easy start of the Docker container by using a shell script.
Easier debugging and tracing by using Redis Stack with built-in RedisInsight for development cycles.
Better security by enabling an access password for ""Production Ready"".
 
- Updated README.md with a Docker for Linux.
- Added the Docker Compose file as `docker_redis.yml`
- Added a shell script to make it easy to start the container with `start_redis_docker.sh`

### Test Plan

<!-- Explain how you tested this functionality. Include the steps to reproduce and any relevant test cases. -->

Configuration validation with `docker-compose -f docker_redis.yml config` on:
- Fedora Linux 37 with Docker version 23.0.3, build 3e7cbfd and Docker Compose version v2.17.2.
- Third party confirmation of correct build on Windows 11 with Docker Desktop 4.18.0 and Docker Compose version v2.17.2.
Multiple build cycles with `docker system prune -a` to eliminate any image conflicts potential

Successful configuration and built of a Docker Stack image and confirmed:
- .env has the correct Redis configuration
- Running Auto-GPT reports ‚ÄúUsing memory of type: RedisMemory‚Äù
- Checking http://localhost:8001/redis-stack/browser and confirming the CPU, Total Memory, Total Keys and Connected Clients all report positive non-zero values.


### Change Safety

- [x] I have added tests to cover my changes
- [x] I have considered potential risks and mitigations for my changes

<!-- If you haven't added tests, please explain why. If you have, check the appropriate box. -->"
13,13,594,"Added some extra instructions to improve PR quality by addressing common pitfalls.

I hope this will improve the quality of PRs and our efficiency in merging and reviewing them.

@Torantulino what do you think?"
14,14,589,"### Background

<!-- Provide a brief overview of why this change is being made. Include any relevant context, prior discussions, or links to relevant issues. -->
Fork and clone instructions are not clear.

### Changes

<!-- Describe the changes made in this pull request. Be specific and detailed. -->
After a fork, the ""git clone"" applies to the newly forked repo, not to origin.

### Test Plan

<!-- Explain how you tested this functionality. Include the steps to reproduce and any relevant test cases. -->
Created a fork/clone for this pull request using the updated instructions.

### Change Safety

- [x] I have added tests to cover my changes
- [x] I have considered potential risks and mitigations for my changes

<!-- If you haven't added tests, please explain why. If you have, check the appropriate box. -->
"
15,15,567,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

That would be simply great instead of using OpenAI

### Examples üåà

_No response_

### Motivation üî¶

_No response_"
16,16,556,"### Background

![image](https://user-images.githubusercontent.com/34168009/230755671-ddc951c8-3566-4426-8587-35a74d7f3186.png)

It's possible for your connection being closed causes AutoGPT to crash.

### Changes

Wraps the embeddings call with a try statement and returns an empty list when finding nearest or avoids adding to memory.

### Test Plan

It's not tested, one could disable their internet connection to test.

### Change Safety

- [ ] I have added tests to cover my changes
- [x] I have considered potential risks and mitigations for my changes

"
17,17,531,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

beable to understand the context of images

### Examples üåà

look on open ais showcase of gpt4 images context understanding and explainion

### Motivation üî¶

want to try out gpt4's image context understanding for myself"
18,18,528,"### System Awareness

- [X] I have searched the existing issues

### Summary üí°

Before going out to look at the internet 
It would be helpful if upon activation the AI took inventory of the system it was on and shared the available tools and capabilities
and if they were insufficient begin researching and developing GAP tools to use during the session with the expressed request to push the GAP tools via PR back to the community

### Examples üåà

AI System initializing
- MacOS 
- Python3
- Pip
- Shell Commands available...
- Desktop App skills available...

What are your goals?

### Motivation üî¶

usuability "
19,19,527,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

I have locally installed Auto-GPT 
within the auto_gpt_workspace - I cloned the Auto-GPT repo
I then asked the AI to look up the outstanding issues and begin to repair them
the AI responded that out of at the time 185 outstanding issues it would focus on issue it could solve quickly

this loop continued as it couldn't seem to write files to the local instance

Version 2

I then asked the AI to search the internet on how it could improve itself 
the AI wanted to install machine learning modules - again it was unable to write to the local repo

I am currently consulting ChatGPT-4 directly to assist this upgrade

### Examples üåà

_No response_

### Motivation üî¶

Self-improving Auto-GPT would be awesome and natural next level"
20,20,526,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

I wrote a python file that i am trying to get auto-gpt to utilize to scrape some data off the internet. To run the file, it requires the use of an argument after the python file. When excecuted, it responds that 'execute_python_file can only execute .py files'. 

### Examples üåà

NEXT ACTION:  COMMAND = execute_python_file ARGUMENTS = {'file': 'name_of_file.py argument'}

expected behavior would be that it runs the python file with the argument and gives the data to auto-gpt

### Motivation üî¶

I noticed auto-gpt struggled with scraping numerical data off the internet so i wrote a python file to find a download button and download the data into .cvs for further data analysis. "
21,21,514,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

When crawling the web to do market research, a lot of links are sometimes just pdf documents. It would be great if Auto GPT had an inherent ability to parse those pdfs & feed the text for GPT4 to analyse.

### Examples üåà

- Research on investing in Emerging Markets in 2023 --> The first few hits on Google Search are pdf documents. Auto GPT fails to parse them.

### Motivation üî¶

This way Auto GPT can do the market research task far better than it currently can."
22,22,513,"### Duplicates

- [X] I have searched the existing issues

### Steps to reproduce üïπ

Ask for Auto GPT to create a image, using sd in the .env

### Current behavior üòØ

The Auto-GPT got this error when tried to use generate_image command: 
Command generate_image returned: Error: cannot identify image file <_io.BytesIO object at 0x000001DF7156E3E0>

### Expected behavior ü§î

Save a image to auto_gpt_workspace folder

### Your prompt üìù

```yaml
ai_goals:
- Create an image of a fox dancing
- shutdown
ai_name: Artist_AI
ai_role: an AI designed to create images
```"
23,23,511,"### Duplicates

- [X] I have searched the existing issues

### Steps to reproduce üïπ

AI Name: GameDev-GPT

Describe your AI's role:  For example, 'an AI designed to autonomously develop and run businesses with the sole goal of increasing your net worth.'
GameDev-GPT is: A game developer able to create browser games with HTML, Javascript and CSS. They use basic graphics and are focused on games played in Firefox on a desktop computer

Enter up to 5 goals for your AI:  For example: Increase net worth, Grow Twitter Account, Develop and manage multiple businesses autonomously'
Enter nothing to load defaults, enter nothing when finished.
Goal 1: Create a game design document for a game of Pong
Goal 2: Write the code for the game
Goal 3: Add unit tests to validate that the code is working
Goal 4: Write all the files to disk
Goal 5: Shutdown when complete


### Current behavior üòØ

The reason it's failing is because it's trying to set the `fillStyle` of the context to `\'white\'` which has a single backslash to it in the output already but fails to get parsed by standard JSON parsers. It should be `'white'` in this context (the code itself is encapsulated between double quotes and thus unaffected by single quotes, yet GPT-4 still added the back-tick most likely due to it recognizing that strings within strings wouldn't work in most cases).

Current full output that broke the system:

`{ ""thoughts"": { ""text"": ""Now that I have defined the game environment and variables, I can start writing the code to update the ball and paddles. This will involve combining the draw and update functions for the ball and paddles so that they canbe updated on each frame. I will also need to add collision detection to ensure that the ball bounces off the paddles and walls as intended."", ""reasoning"": ""By combining the draw and update functions into a single function for each object, I can simplify the code and better manage the game state. Adding collision detection is crucial to ensure that the gameplay is accurate and consistent."", ""plan"": ""- Combine the ball and paddle draw and update functions\n- Add collision detection for paddles and walls\n- Test the game mechanics thoroughly"", ""criticism"": ""I should be careful to test each addition thoroughly to make sure that they are working as intended. I should also be aware of potential issues that may arise from combining the draw and update functions and plan accordingly."", ""speak"": ""Now it's time to update the ball and paddles in the game environment. I'll combine the draw and update functions for each object and add collision detection to ensure that the gameplay isaccurate. I'll be sure to thoroughly test each addition as I go."" }, ""command"": { ""name"":""write_to_file"", ""args"": { ""file"":""pong_code.js"", ""text"": ""// Pong Game Code\n\n// Global variables\nvar canvas = document.getElementById('game-canvas');\nvarcontext = canvas.getContext('2d');\n\nvar ball = {\n x: canvas.width/2,\n y: canvas.height/2,\n dx: 5,\n dy: 5,\n radius: 10\n};\nvar paddle1 = {\n x: 50,\n y: canvas.height/2 - 70,\n width: 10,\n height: 140,\n dy: 0\n};\nvar paddle2 = {\n x: canvas.width - 60,\n y: canvas.height/2 - 70,\n width: 10,\n height: 140,\n dy: 0\n};\n\n// Ball functions\nfunction updateBall(){\n ball.x += ball.dx;\n ball.y += ball.dy;\n // Check for wall collisions\n if(ball.y + ball.radius > canvas.height || ball.y - ball.radius < 0){\n ball.dy = - ball.dy;\n }\n // Check for paddle collisions\n if(ball.x + ball.radius > paddle2.x && ball.y > paddle2.y && ball.y < paddle2.y + paddle2.height || ball.x - ball.radius < paddle1.x + paddle1.width && ball.y> paddle1.y && ball.y < paddle1.y + paddle1.height){\n ball.dx = - ball.dx;\n }\n}\n\nfunction drawBall(){\n context.beginPath();\n context.arc(ball.x, ball.y, ball.radius, 0, Math.PI*2);\n context.fillStyle = \\'white\\';\n context.fill();\n context.closePath();\n}\n\n// Paddle functions\nfunction updatePaddle1(){\n paddle1.y += paddle1.dy;\n // Check for wall collisions\n if(paddle1.y + paddle1.height > canvas.height || paddle1.y < 0){\n paddle1.dy = - paddle1.dy;\n }\n}\n\nfunction updatePaddle2(){\n paddle2.y += paddle2.dy;\n // Check for wall collisions\n if(paddle2.y + paddle2.height > canvas.height || paddle2.y < 0){\n paddle2.dy = - paddle2.dy;\n }\n}\n\nfunction drawPaddle1(){\n context.beginPath();\n context.rect(paddle1.x, paddle1.y, paddle1.width, paddle1.height);\n context.fillStyle = \'white\';\n context.fill();\n context.closePath();\n}\n\nfunction drawPaddle2(){\n context.beginPath();\n context.rect(paddle2.x, paddle2.y, paddle2.width, paddle2.height);\n context.fillStyle = \\'white\\';\n context.fill();\n context.closePath();\n}\n\n// Main function\nfunction mainLoop(){\n context.clearRect(0, 0, canvas.width, canvas.height);\n updateBall();\n updatePaddle1();\n updatePaddle2();\n drawBall();\n drawPaddle1();\n drawPaddle2();\n}\n\n// Event Listeners\ndocument.addEventListener('keydown', function(event) {\n // Listening for keyboard inputs here\n});\n"" } } }`

There are 3 instances where `\'white\'` is being used, changing those to `'white'` fixes the JSON issue. 

### Expected behavior ü§î

That GPT-4 doesn't output invalid JSON data when writing code.

There's two ways I see this could be fixed, one would be by adjusting the input prompt to tell it not to add a `\` character to single quotes. The other way would be to add an extra step for when the JSON decoding fails to check and see if there may be instances of `\'` and resolve them (potentially ask the user as well to be sure this could be the fix for the current output)

### Your prompt üìù

All prompts used written above, using commit `7e108d8ab37d828c3d7b129399d8d9927154f801` which is the latest at this point in time for me."
24,24,510,"### Background

Big branch build on top of https://github.com/Torantulino/Auto-GPT/pull/372 (Redis implementation and memory refactor).

We need to improve modularity to scale the project so I created AutoGPT wrapper with the old main.py functionality.

I also added a PydanticParser similar to Langchain with some improvements.

### Changes

- AutoGPT wrapper to instantiate new execution and scale the project (we can add layers on top of single executions).

- PydanticParser to fix the error parsing the JSON and return a notification to the AI pointing the error and asking for a fix with a specific format. (E.g: ```Failed: 
 ERROR = I couldn't parse your format for object: Command, remember you should follow the instructions: The output should be formatted as a JSON instance that conforms to the JSON schema below. As an example, for the schema {""properties"": {""foo"": {""title"": ""Foo"", ""description"": ""a list of strings"", ""type"": ""array"", ""items"": {""type"": ""string""}}}, ""required"": [""foo""]}} the object {""foo"": [""bar"", ""baz""]} is a well-formatted instance of the schema. The object {""properties"": {""foo"": [""bar"", ""baz""]}} is not well-formatted. Here is the output schema: {""Command"": {""name"": {""title"": ""Name"", ""description"": ""command name"", ""type"": ""string""}, ""args"": {""title"": ""Args"", ""description"": ""A dictionary where keys and values are both strings, e.g., {'arg1': 'value1', 'arg2': 'value2'}"", ""type"": ""object"", ""additionalProperties"": {""type"": ""string""}}}} ```

- ResponsePrompt a modular prompt for our system which uses default values (and AI specific settings) for now but can be easily reconstructed sending new values (E.g Commands: {new small list of commands for specific task}, Constraints: {the constraints for this agent}.

### Test Plan

Preparing a test for the pydantic parser and the Response prompt formatting.

### Change Safety

I tested my implementation with 20x automatic executions and now we are able to recover from wrong format.
"
25,25,506,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

Auto-GPT needs an inherent understanding of itself and the ability to connect with other Auto-GPT instances for collaboration. Once we get them talking they can figure out the most efficient way to move forward. 

I was thinking we could have people run instances donating their API key on running an Auto-GPT cluster focused on improving Auto-GPT and implementing the latest technologies and capabilities into itself. 

### Examples üåà

_No response_

### Motivation üî¶

Speed."
26,26,503,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

Replace Google Search by an agency (coop of agents) to search both Google and pinecone memory from a prompt sent by autogpt to this agency 

Uses multiple instances of a GPT-3.5 model to generate tags for each search engine (e.g., Pinecone and Google) based on the user prompt.
Uses the generated tags to query Pinecone database and Google Search API for the top results relevant to the user prompt.
Uses a GPT-3.5 summarization and organization model to identify the top 5 most relevant search results from Pinecone database and Google Search API, respectively.
Uses a GPT-3.5 summarization and organization model to summarize and organize the search results from Pinecone database and Google Search API in relevance to the user prompt.
Combines the summarized and organized search results from both sources using a GPT-3.5 model.
Outputs it to autogpt 


### Examples üåà

_No response_

### Motivation üî¶

_No response_"
27,27,500,"[![workerB](https://img.shields.io/endpoint?url=https%3A%2F%2Fworkerb.linearb.io%2Fv2%2Fbadge%2Fprivate%2FU2FsdGVkX12X34Yic5DPfPZec1mXyQl10i0d9YJs%2Fcollaboration.svg%3FcacheSeconds%3D60)](https://workerb.linearb.io/v2/badge/collaboration-page?magicLinkId=e2NUf8l)
### Background

<!-- Provide a brief overview of why this change is being made. Include any relevant context, prior discussions, or links to relevant issues. -->

Discussion created here https://github.com/Torantulino/Auto-GPT/issues/492
One topic i would like to raise is the use of optional arguments, as Auto seems to ask for it anyway. Maybe we can add a kind of optional flag in the prompt.

### Changes

<!-- Describe the changes made in this pull request. Be specific and detailed. -->
I have added an optional argument and fallback to default if not set.
<img width=""795"" alt=""image"" src=""https://user-images.githubusercontent.com/78359496/230732622-17939d23-745b-45c1-af5f-444ad1db550f.png"">


### Test Plan
Use a prompt that will trigger the ""generate_image"" command and specify in natural language an image size.
<!-- Explain how you tested this functionality. Include the steps to reproduce and any relevant test cases. -->

### Change Safety
Only 3 sizes available at the moment, 256, 512, 1024 and with proper format, else fallback to 256x256.

- [x] I have added tests to cover my changes
- [x] I have considered potential risks and mitigations for my changes

<!-- If you haven't added tests, please explain why. If you have, check the appropriate box. -->
"
28,28,495,"### Design Document
 https://docs.google.com/document/d/1pHtgK4OaFxy2POColR4nYJ3wemU4XLTEoxTLQunzaU0/edit?usp=sharing

### Background

<!-- Provide a brief overview of why this change is being made. Include any relevant context, prior discussions, or links to relevant issues. -->

One issue I have seen while experimenting with AutoGPT is that it will go deep down irrelevant rabbit holes. In order to address this I am experimenting with having a more structured way of organizing tasks.

### Changes

<!-- Describe the changes made in this pull request. Be specific and detailed. -->
- `TaskManager`: A singleton class responsible for managing tasks, including creating tasks, retrieving tasks by ID, updating task states, and displaying task hierarchy.
- `TaskState`:An enumeration representing the state of a task (NOT_STARTED, IN_PROGRESS, COMPLETED).
- `Task`: A class representing individual tasks with properties like ID, name, state, parent task, and subtasks.
- New `--taskmanager` command line flag to enable and disable
- New commands for managing tasks: `update_task_state` and `set_current_task`

### Test Plan

<!-- Explain how you tested this functionality. Include the steps to reproduce and any relevant test cases. -->

- Add unit tests for Task Manager, Task, and TaskState

### Outstanding items

- Refine prompt to ensure AutoGPT is always managing tasks
- Allow for programmatically adding task manager portion of prompt based off command line flag

### Change Safety

- [ ] I have added tests to cover my changes
- [ ] I have considered potential risks and mitigations for my changes

<!-- If you haven't added tests, please explain why. If you have, check the appropriate box. -->
"
29,29,492,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

Command Generate Image ""generate_image"" is a great addition. However the image size is hardcoded,
I propose the issue to make it controllable with an optional argument image_size.

Pitfalls : 

- Dalle has only 3 sizes ""256x256"", ""512x512"", ""1024x1024"".
- StableDiffusion works with multiple of 8 as valid size, but image may suffer loss of context > 512px.
- image_size argument should be optional and not make it more complicated to generate an image without specifying the size. 

### Examples üåà

- Stable Diffusion Replica, size can be changed
https://replicate.com/stability-ai/stable-diffusion

- Dall-e API
https://platform.openai.com/docs/api-reference/images/create#images/create-size


### Motivation üî¶

This will help the user to generate an image at the needed size.

I will push a PR in that direction."
30,30,453,"### Background

Felt I need more optionality at start-up: load last settings, load setting file from your history or create new settings. The need came out of running different ""missions"" with changes to the settings and prompts. Wanted an easy way to recall previous settings 

### Changes

See new onboarding flow here: https://user-images.githubusercontent.com/1012888/230700981-90509b51-c38a-47d6-96bf-47519c91c6d3.mov

- Created new structure in scripts/data:
```bash
scripts/data/ai_settings
scripts/data/ai_settings/last_settings.yaml
scripts/data/ai_settings/history
scripts/data/ai_settings/history/settings_20230407-195943.yaml [every setting that is run is saved in the history dir]
```
- Refactored `main.py/construct_prompt` to support 3 onboarding options:
```bash
Welcome! Please select one of the following options:
a - Load last used settings
b - Load another settings file
c - Create new settings
```
- Refactored ai_config to support new i/o stuff


### Test Plan

- Manual testing throw each onboarding flow and i/o operation

### Change Safety

- [ ] I have added tests to cover my changes
- [ ] I have considered potential risks and mitigations for my changes




<!-- If you haven't added tests, please explain why. If you have, check the appropriate box. -->
"
31,31,442,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

Looking at Microsoft‚Äôs Jarvis and the improvement seen with refinement (where GPT is prompted after it‚Äôs response ‚Äúdid you do the last task correctly?‚Äù). Perhaps we could implement a refinement check before proceeding on a task, and use some sort of sentiment analysis from hugging face to then turn that into a Boolean of continue or refine. Then if it still fails after 3 or so retries it could ask a question back to the user for help where it got stuck. 

### Examples üåà

Jarvis integration with hugging face https://github.com/microsoft/JARVIS

Paper on refinement technique: https://arxiv.org/pdf/2303.11366.pdf?utm_source=aibreakfast.beehiiv.com&utm_medium=referral&utm_campaign=why-every-company-will-use-chatgpt-s-retrieval

### Motivation üî¶

The goal would be to trade speed for accuracy in an automated fashion for difficult problems. Some tasks might require higher accuracy and this could be a way to improve the results without waiting for a more sophisticated model."
32,32,430,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

Any thoughts on enabling auto-GPT to pick up where it left off after an error? It has done some noble work, but has never fully completed the tasks I've set for it before getting cut off by this kind of error:
  raise error.APIConnectionError(
openai.error.APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))

### Examples üåà

_No response_

### Motivation üî¶

_No response_"
33,33,399,"### Duplicates

- [X] I have searched the existing issues

### Steps to reproduce üïπ

I'm not sure how to reproduce it, i've had it running trying to make a crossword generator for a while.

### Current behavior üòØ

The PLAN: output shows this:
-  Add a text entry box function for player input\n- Create a function to validate the user's input\n- Compare the user input to the correct answer and display the correct answer if necessary

### Expected behavior ü§î

It should show this.
- Add a text entry box function for player input
- Create a function to validate the user's input
- Compare the user input to the correct answer and display the correct answer if necessary

### Your prompt üìù

I can't find the last_run_ai_settings.yaml, however it seemed to remember my prompt from last time. I had some problems with the application crashing several times, before i got it working.
"
34,34,393,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

My current challenge is it's hard to know how much it's costing me to run autoGPT for each session (for OPEN_AI in particular)

If I knew how much it was costing me and the current cost per session I could make informed decisions about running in continuous mode or authorizing discrete tasks.

First it would be great just to see estimated costs per session.

Second, I would like to be able to set budget for each session with a soft/hard cap.

For example a soft cap of $2 would mean it leaves continuous mode when it nears $2 estimated cost and you see a warning.
A hard cap would end the session for me, but is optional.
If neither are provided it runs as expected.

Thanks,
Tim


### Examples üåà

_No response_

### Motivation üî¶

_No response_"
35,35,389,"### Duplicates

- [X] I have searched the existing issues

### Steps to reproduce üïπ

Unsure, it randomly seems to produce this error. I've even given instructions that if the AI failed to parse the output then to reformat the input to a manner which the AI can parse. 


Warning: Failed to parse AI output, attempting to fix.
 If you see this warning frequently, it's likely that your prompt is confusing the AI. Try changing it up slightly.
Failed to fix ai output, telling the AI.
NEXT ACTION:  COMMAND = Error: ARGUMENTS = string indices must be integers, not 'str'
SYSTEM:  Command Error: returned: Unknown command Error:

### Current behavior üòØ

Randomly it will return this error when given a prompt or running multiple times. 

### Expected behavior ü§î

Should not have a type error of uint or str. 

### Your prompt üìù

``Describe your AI's role:  For example, 'an AI designed to autonomously develop and run businesses with the sole goal of increasing your net worth.'
Shopify and wordpress site finder is: Search the internet for a list of 100 shopify and wordpress websites that are popular and then use that list to perform extra validation that will be given to you once the list is finalized. CRITICAL! You may only respond in JSON format as described below. Go carefully and step by step. Do not output any additional instructions outside of the JSON format. Under no circumstances should your response deviate from valid JSON format.
Enter up to 5 goals for your AI:  For example: Increase net worth, Grow Twitter Account, Develop and manage multiple businesses autonomously'
Enter nothing to load defaults, enter nothing when finished.
Goal 1: Search for 100 wordpress and shopify sites online that are popular and have high traffic.
Goal 2: Compile the list into a set of URLs
Goal 3: Try to find contact information for each of the websites
Goal 4: Ask for more instructions once the list has been compiled and saved as a .txt file
Goal 5:
Using memory of type: PineconeMemory
SHOPIFY AND WORDPRESS SITE FINDER THOUGHTS: I need to start by searching for popular Shopify and Wordpress websites. I can use Google search to find a list of the top websites.
REASONING: Searching Google for lists of popular Shopify and Wordpress sites is the best way to get a comprehensive list.
PLAN:
-  Use Google Search to search for lists of popular Shopify and Wordpress websites
-  Compile a list of websites and URLs
-  Locate contact information for each website
-  Await further instructions for additional validation
CRITICISM: I need to be efficient while searching for websites to make the best use of my resources.
NEXT ACTION:  COMMAND = google ARGUMENTS = {'input': 'popular Shopify Wordpress websites'}
Enter 'y' to authorise command, 'y -N' to run N continuous commands, 'n' to exit program, or enter feedback for Shopify and wordpress site finder...
Input:y
```"
36,36,386,"### Background

I am using Auto-GPT search, browse and summarize code in another project and using this parallelization of single url summarization and summarizing multiple urls. 
As most of the processing is IO bound I find this approach to be useful.


### Changes

* Added ChatRequest class which encapsulates openai.ChatCompletion parameters and adds an optional key to requests.
* Added ChatResponse class which encapsulates a ChatRequest and its reply (or Exception)
* Added async_chat_completion to llm_utils with ChatRequest input ChatResponse output
* Changed summarize_text to be async, parallelizing chunks summarizations.
* Created a search file and moved relevant functions into it from commands
* Made get_text_summary async. Can be used to summarize multiple urls in parallel depending on num_sources requested (default is 1)
* Moved main loop into an async function to be asyncio.run from  main.
* Had an issue with Pinecone memory clear() so put it in try/except

### Test Plan

test_search - test_search_and_summarize_multiple_sources get 5 links from google_official_search and creates summaries from 3. It fails on the NYT and generates the third summary from the 4th url. (can't really build on this flow)


### Change Safety

- [x] I have added tests to cover my changes
- [x] I have considered potential risks and mitigations for my changes

I ran main with --continuous for a while it was stable.

"
37,37,375,"### Background

Devs and aspiring contributors frequently have dependency conflicts when trying to clone the repo and build dependencies for the first time. 

Adding an alternate requirements file:
- Removes package versions, allowing pip to resolve dependency conflicts with less strict versioning
- Explicitly adds packages to be installed, that was being missed by pip and resulting in 'module not found' errors


### Changes

Add a new requirements-alternate.txt

### Test Plan

Other contributors and myself have found success with this requirements file, as opposed to the main one.

### Change Safety

- [x] I have added tests to cover my changes
- [x] I have considered potential risks and mitigations for my changes

<!-- If you haven't added tests, please explain why. If you have, check the appropriate box. -->
"
38,38,368,"### Duplicates

- [X] I have searched the existing issues

### Steps to reproduce üïπ

N/A

### Current behavior üòØ

Auto-GPT has limited test coverage, which makes it difficult to ensure that changes to the codebase do not introduce new bugs or regressions. 

Given the [latest commit](https://github.com/Torantulino/Auto-GPT/commit/c6d90227fecec8acc1481c486a91337b07e8a820), testing is now part of all new pull requests.

### Expected behavior ü§î

By improving test coverage, we can ensure that the codebase is more robust and that future changes can be made with confidence.

### Your prompt üìù

N/A (since this issue is not related to a specific prompt)"
39,39,364,"### Background
Hey everyone, I saw some Twitter posts about people being able to install react apps and what not but didn't seem to be able to get it to work on my end. So, I took a stab at creating a module for working with react apps and adding github versioning functions. If anyone has any input or is willing to help me test this please ping me! 

Don't know how safe it is to update the prompt either, seems to ""on the nose""

### Changes
This pull request adds the following changes:

- Added react_operations.py module for React app management
- Added git_operations.py module for GitHub repository management
- Added environment variables for GitHub username and PAT in the .env file
- Updated the execute_command() function to include new React and GitHub commands in the AI agent prompt

### Test Plan
WIP(will circle back on this on Friday evening)

### Change Safety

- [ ] I have added tests to cover my changes
- [ ] I have considered potential risks and mitigations for my changes

<!-- If you haven't added tests, please explain why. If you have, check the appropriate box. -->
"
40,40,356,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

I'm wondering whether Auto-GPT, even if it's written in Python, could support other programming languages as well, since GPT is able to program in multiple ones.

I'm also thinking that Auto-GPT could be more effective in languages with a good type system, like Rust or Haskell. That's because function signatures give a lot of information to understand what a function does: we could be very efficient in token usage, if we transmitted function signatures without a function body.

### Examples üåà

_No response_

### Motivation üî¶

_No response_"
41,41,348,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

Implement ""Fully Air-Gapped Offline Auto-GPT"" functionality that allows users to run Auto-GPT without any internet connection, relying on local models and embeddings.

This feature depends on the completion of the feature requests [#347 Support Local Embeddings](https://github.com/Torantulino/Auto-GPT/issues/347) and [#25 Suggestion: Add ability to choose LLM endpoint](https://github.com/Torantulino/Auto-GPT/issues/25).

### Examples üåà

1. Secure facilities with strict data access controls can utilize Auto-GPT without risking sensitive information leaks.
2. Researchers working in remote locations without internet access can continue to experiment with and use Auto-GPT.
3. Users who prefer not to rely on cloud services can run Auto-GPT entirely within their local environment.
4. Robots can make decisions and communicate using LLMs even when they can't connect to the internet.

### Motivation üî¶

1. Enhanced Security: Fully air-gapped Auto-GPT ensures that no sensitive data is transmitted over the internet, protecting it from potential leaks or unauthorized access.
2. Offline Capabilities: This feature allows users to leverage Auto-GPT's power in environments with limited or no internet access, expanding its potential use cases.
3. Reduced Dependency: By removing reliance on external servers, users gain greater control over their data and can manage resources more efficiently.
4. Cost Savings: By not requiring cloud-based services, users can save on costs related to storage, data transfer, and processing.
5. Customization and Control: Users can manage their own models and embeddings, tailoring the system to their specific needs and preferences. "
42,42,347,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

Local Models (LLaMA & its finetunes) now work in a fork of Auto-GPT, including with Pinecone Embeddings. See https://github.com/Torantulino/Auto-GPT/issues/25#issuecomment-1499531177

Local models and embeddings offer better privacy, lower costs, and enable new uses, like Auto-GPT experiments in private/air-gapped networks. To get these benefits, we should add local (offline) embeddings storage and recall to Auto-GPT.

### Examples üåà

A version of ooba's text-generation-webui, called [wawawario2/long_term_memory](https://github.com/wawawario2/long_term_memory), has done this using zarr and Numpy. Check [wawawario2/long_term_memory#how-it-works-behind-the-scenes](https://github.com/wawawario2/long_term_memory#how-it-works-behind-the-scenes)

![image](https://user-images.githubusercontent.com/5949853/230487779-d6748718-8ee0-49b6-9026-6758b48deaae.png)

Though the Auto-GPT fork uses ooba's webui API for local models, the long_term_memory project is closely tied to ooba's UI. We mention it only as a reference. We need to create a similar setup in Auto-GPT.

### Motivation üî¶

1. Better Privacy: Local embeddings keep user data on their devices or networks, avoiding the risk of online data breaches.
2. Lower Costs: Users save money on cloud storage, bandwidth, and processing power when they use local embeddings.
3. Offline Use: Auto-GPT can work in offline environments or areas with limited internet, like secure facilities or remote research stations.
4. Customization: Users can create and manage their own embeddings for better model performance and results.
5. Faster Response: Local embeddings can speed up response times since data doesn't need to travel to remote servers.

By adding local embeddings storage and recall to Auto-GPT, users get more control, flexibility, and benefits like privacy, cost savings, and accessibility."
43,43,346,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

1. Attach to a VirtualBox instance, give AI a default OS like ubuntu
2. if AI decide to use computer: enter ""screenshot-mouse/keyboard"" loop, also ai can dump file into or grab file from the virtual machine.


### Examples üåà

_No response_

### Motivation üî¶

it allows ai to do more human like tasks, alsoI think this could also solve crawling problem, because ai can somehow ""see"" the webpage."
44,44,344,I've added one other env variable to set tha pinecone index/table name from envs; previously it was a bit unclear how to configure pinecone and where does it get the table name from
45,45,340,"Just for consistency, cost me ~5 seconds at setup ü§ñ "
46,46,335,
47,47,324,"Test, and make the needed changes to get this to work.

It may or may not work, there are still more things needed to get it working, but from testing it may work.

Possible Suggestions:

1) Combine the output of the custom model and gpt-3.5 or gpt-4 for thoughts, reasoning, criticism
2) Fully custom.
"
48,48,319,"### Duplicates

- [X] I have searched the existing issues

### Steps to reproduce üïπ

Upgraded to latest version with Pinecone. Prompts that worked great yesterday now break constantly. Throwing the error:

Failed to parse AI output, attempting to fix.
 If you see this warning frequently, it's likely that your prompt is confusing the AI. Try changing it up slightly.
 
 I'm using GPT3.5.

### Current behavior üòØ

_No response_

### Expected behavior ü§î

_No response_

### Your prompt üìù

designAI

An AI created to help with design direction for a SaaS company

Research 2023 graphic design trends

Take 10 of the trends, and list them with description in a csv file"
49,49,316,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

Idea:

- Add a `ai_settings` directory to contain yaml files in the form of `last_run_ai_settings.yaml`. This way we could create and share interesting or successful ""missions."" 
- Update `scripts/main/parse_arguments` so you can easily load any of the `...ai_settings.yaml` files in the `ai_settings` directory.

Planning to add this to my local fork. Happy to do a PR if others like the idea :)

ps - awesome tool! thanks for sharing with the community!

### Examples üåà

_No response_

### Motivation üî¶

_No response_"
50,50,305,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

Summarize and then critique current approach and progress. I have done x,y and z. Progress p has been made toward the goal. Changing the approach is/is not warranted. Plan amendments etc.



### Examples üåà

_No response_

### Motivation üî¶

If we get caught in a loop, gpt is definitely capable of stopping itself, but doesn't always seem to at the moment."
51,51,304,"### Duplicates

- [X] I have searched the existing issues

### Summary üí°

Right now I am experimenting with a Wikidata Query command in auto-gpt on my repo: https://github.com/stefangrotz/Auto-GPT-Wikidata

Feel free to join!

Once this works well I can create a pull request and add this functionality to this repo. Would this make sense to you? 

At one point some sort of plugin system will be inevitable, but Wikidata could be usefull enought to become part of the main system since it gives you acces to all kind of data in a structured way. This includes lists of API endpoints of Open Data CKAN Portals and many other useful data that is hard to find just using google.

### Examples üåà

Here is an example how this can look:
![image](https://user-images.githubusercontent.com/5982239/230336137-f8dc6707-2a45-42ab-a745-2ca4c6419e36.png)


### Motivation üî¶
Wikidata ftw"
52,52,298,"So there are a few issues here that going into a larger design discussion so for now I re-worked the prompts, added a bit more option there and created a fallback output if the LLM decides to break format. 

This also removes the LLM cycle where it tries to fix its own output. Def would love to play with that again for sure. The hardcoded ""you are naughty dont break format again"" should do the trick for now. 

Learned something doing this one, you cant give it an ounce of an idea a human is at the console, it just overrides the world. You cant even use words like ""human"", breaks it right out of the box. "
53,53,296,"error During Requirement .txt execution 

On raspberry pi 4b 8GB with Raspbian OS 

Python 3.7.3
pip3 install -r requirements.txt

error: 
  Could not find a version that satisfies the requirement python-dotenv==1.0.0 (from -r requirements.txt (line 5)) (from versions: 0.1.0, 0.1.2, 0.1.3, 0.1.5, 0.2.0, 0.3.0, 0.4.0, 0.5.0, 0.5.1, 0.6.0, 0.6.1, 0.6.2, 0.6.3, 0.6.4, 0.6.5, 0.7.0, 0.7.1, 0.8.0, 0.8.1, 0.8.2, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.10.2, 0.10.3, 0.10.4, 0.10.5, 0.11.0, 0.12.0, 0.13.0, 0.14.0, 0.15.0, 0.16.0, 0.17.0, 0.17.1, 0.18.0, 0.19.0, 0.19.1, 0.19.2, 0.20.0, 0.21.0, 0.21.1)
No matching distribution found for python-dotenv==1.0.0 (from -r requirements.txt (line 5))


Python 2.7.16
pip install -r requirements.txt

Error : 
Could not find a version that satisfies the requirement openai==0.27.2 (from -r requirements.txt (line 3)) (from versions: 0.0.2, 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.2.0, 0.2.1, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.3.0, 0.4.0, 0.6.0, 0.6.1, 0.6.2, 0.6.3, 0.6.4)
No matching distribution found for openai==0.27.2 (from -r requirements.txt (line 3))
"
54,54,292,"Congratulation for this amazing Project 
i have local p retrained model through https://github.com/oobabooga/text-generation-webui
using their API in this format ""

import requests

response = requests.post(""http://127.0.0.1:7860/run/textgen"", json={
	""data"": [
		""Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction:
Write a poem about the transformers Python library. 
Mention the word ""large language models"" in that poem.
### Response:
"",
		200,
		False,
		1.99,
		0.18,
		1,
		1.15,
		1,
		30,
		0,
		0,
		1,
		0,
		1,
		False,
		-1,
	]
}).json()

data = response[""data""]

looking forward to be able to use local API instead of openAI one

any suggestion for the required modification that i can work on

Regards

"
55,55,289,Addresses #270 
56,56,286,"I've encountered the same problem as #101. GPT Thinks that `evaluate_code` will execute python:
```
CODEREVIEW-GPT THOUGHTS: Let's find some code to review and suggest improvements.
REASONING: We need to search for files with common code extensions that are not too large and more recent than a certain date.
PLAN:
-  Use the 'evaluate_code' command to run a Python script that searches for files with common code extensions in the current folder and subfolders.
-  Use the 'memory_add' command to save the file paths for later reference and reuse.
-  Review the saved files using 'read_file' command to ensure they are indeed code files.
-  Choose a file to review and suggest improvements.
CRITICISM: We may need to adjust the search criteria if we don't find relevant code or if too many files match the criteria.
NEXT ACTION:  COMMAND = evaluate_code ARGUMENTS = {'code': 'import pathlib\nfrom typing import List\n\ndef find_code_files(root: str, extensions: List[str], max_size: int, min_date_modified: float) -> List[str]:\n """"""\n Returns a list of file paths for code files with the specified extensions, maximum size, and minimum date modified\n rooted in the given directory and its subdirectories.\n """"""\n files = [str(path) for path in pathlib.Path(root).rglob(""*"") if path.suffix in extensions\n and path.stat().st_size <= max_size\n and path.stat().st_mtime >= min_date_modified]\n return files\n\ndef print_files(files: List[str])
 -> None:\n """"""\n Prints the list of file paths to the console, one file per line.\n If no relevant files are found, prints a message indicating so.\n """"""\n for file in files:\n print(file)\n else:\n print(""No
 relevant files found."")\n\n\ndef analyze_code(code: str) -> None:\n """"""\n Analyzes the given code and provides suggestions for improvements.\n """"""\n pass\n\nif __name__ == \'__main__\':\n root: str = \'.\'\n extensions: List[str] = [\'.py\', \'.js\', \'.java\']\n max_size: int = 10000 # bytes\n min_date_modified: float = 1609459200 # seconds since epoch (01/01/2021)\n\n files: List[str] = find_code_files(root, extensions, max_size, min_date_modified)\n print_files(files)\n'}
```

On one hand, that's a bug, which is addressed on #101.

On the other hand... that's a very interesting idea from GPT. Perhaps giving it the ability to execute python code could allow it to execute a lot of tasks in a way more dynamic way."
57,57,285,"Fixed the json_parser.py error by making chatgpt fix itself

def fix_and_parse_json(json_str: str, try_to_fix_with_gpt: bool = True):
    json_schema = """"""
    {
    ""command"": {
        ""name"": ""command name"",
        ""args"":{
            ""arg name"": ""value""
        }
    },
    ""thoughts"":
    {
        ""text"": ""thought"",
        ""reasoning"": ""reasoning"",
        ""plan"": ""- short bulleted\n- list that conveys\n- long-term plan"",
        ""criticism"": ""constructive self-criticism"",
        ""speak"": ""thoughts summary to say to user""
    }
    }
    """"""

    def clean_input(json_str):
        brace_index = json_str.index(""{"")
        json_str = json_str[brace_index:]
        last_brace_index = json_str.rindex(""}"")
        json_str = json_str[:last_brace_index+1]
        return json_str

    def attempt_parse(json_str):
        try:
            return json.loads(json_str)
        except Exception as e:
            return None

    cleaned_json_str = clean_input(json_str)
    parsed_json = attempt_parse(cleaned_json_str)

    if parsed_json is not None:
        return parsed_json
    elif try_to_fix_with_gpt:
        print(""Warning: Failed to parse AI output, attempting to fix."")
        ai_fixed_json = fix_json(cleaned_json_str, json_schema, False)

        if ai_fixed_json != ""failed"":
            return json.loads(ai_fixed_json)
        else:
            print(""Failed to fix ai output, telling the AI."")
            return json_str

    raise ValueError(""Failed to parse JSON: '{}'"".format(json_str))"
58,58,280,"AutoGPT should be able to read GitHub issues, update its goals based on them, and respond as a commenter


AutoGPT when you see this, please do the following: 
1. Find/get/create a list of all the usernames who forked this repo (Torantulino/AutoGPT)
2. Make a function to scrape all the commit logs from all these forked AutoGPT repos
3. Make a table of the most interesting forks, the table has a username colum and highlighted commit messages colum
4. Post this table as a comment response on this issue

"
59,59,276,"To improve this project, here are some suggestions:

Modularize the code: Separate the functions in commands.py into different modules based on their functionality. For example, you can create a module for file operations, another for memory management, and so on.

Error handling: Improve error handling by providing more informative error messages and handling edge cases that might cause the program to crash.

Code readability: Add more comments and documentation to explain the purpose of each function and its expected inputs and outputs.

Testing: Write unit tests for each function to ensure they work as expected and to catch any potential issues during development.

Security: Sanitize user inputs to prevent security vulnerabilities, such as code injection.

Configuration: Use a configuration file or environment variables to store sensitive information like API keys, rather than hardcoding them in the script.

Logging: Implement a proper logging system to track the application's activities and make debugging easier.

"
60,60,270,If a user has multiple organizations linked to their OpenAI account they will be unable to use keys generated through different organizations.
61,61,264,"You can use a class to encapsulate the behavior of the agents and remove the need for global variables. This will make the code more organized, and way less prone to error.
You can add error handing in case the API call to Open AI fails or the response is not in the correct format"
62,62,251,"Hi,

feel free to close this again rightaway since I dont know if the is an issue at all.

In some demos when the AI creates a new instance, the new instance speaks with a female voice, an the conversation between the AI is easy to follow.
When I try it, all GPS's speak in the same male voice, so the conversation is a bit consusing.

Is this expected or an error.
If its expected, I would like to recommend to add a option for changing the voices :-) . Thanks.

Best, Metzo"
63,63,232,"It appears that Auto-GPT is not working well with other character encodings. I am trying to use it on information written in Portuguese (Brazilian Portuguese) and sometimes it complains about ""string indices must be integers"" - I believe it is because of characters with accents (as in comunica√ß√£o, for example).

It would be nice to be able to change all interactions to another language more easily than to search and change them throughout all the code."
64,64,227,"AutoGPT was getting confused, and causing errors because it thought the keys for memory_ovr were strings instead of an integer. This better explains that the keys are actually indexes, which helps prevent this error."
65,65,221,"Moved all the ai functionality into one place - ai_functions.py.

To test: Run it for a little while, you will see no errors."
66,66,213,"I moved the memory functionality into it's own file, and made it a singleton. Works as expected."
67,67,203,"Hi there,
since this is an evolving system, i think we should use collective experience, and like a tesla fleet, get better collectively
### Proposal: Record Insights, Submit for review, Use Pinecone Embeddings
if a task is requested and it takes it X iterations to solve it, what if we can make a report of insights might save us steps in the future, should we try again, or advise others to replicate. 
There are 7k+ people who started and might be using Auto-GPT, together, the learning curve can be increadible
One important thing to remember, is to check each submit and flag potentially harmful insite reports
Then we need to come up with a rating mechanism, like stars, flags, something like stackoverflow, that agents can mark if some insight was helpful, or not helpful, etc, and this database of solutions/insights will be a fluid and evolving knowledge base
might even be wise to allocate ""repositories"" or ""namespaces"" that can be verified by the comunity, the repo can be a client to a public pinecone server, that is managed and verified by the comunity and only accepts verified insight submissions

Like a good fleet, we need to share with eachother all the safe and useful progress, and flag bad practices

If you get my drift, please tag relevant people you think are up for it
If you're worried about token costs, remember this might save multi steps, and also we should have gpt4all soon, and it can also just be a powerful opt-in

Would love to know your thoughts on this feature
Thanks and have a good one!

"
68,68,201,"This change fixes an issue where we were writing the `string` as the `key` in `commit_memory`

it re-designs perm memory to be a dictionary (key, value store) that seems to be a better fit for our use-case


test logs:

```
SYSTEM:  Command memory_add returned: Committing memory with key: pydantic_info, string ""Pydantic models are classes that can be used to convert and validate Python data. They are used extensively in FastAPI to define the data input and output of API calls.""
```"
69,69,182,"raise error.APIConnectionError(
openai.error.APIConnectionError: Error communicating with OpenAI: HTTPSConnectionPool(host='api.openai.com', port=443): Max retries exceeded with url: /v1/chat/completions (Caused by ProxyError('Cannot connect to proxy.', OSError(0, 'Error')))"
70,70,181,"Work in progress, but im Trying to give auto gpt a way to build, manage, and run command inside docker containers.

The goal are : 
- No need tp set up any dev environnment for Auto-Gpt to work
- A way for Auto-GPT to run any commands in a controlled environnement
- A way for Auto-GPT to develop complete apps using the virtual docker environment.

Im not sure about : 
- The prompts given to explain the commands
- The goal i gave auto gpt

Here is the reference goal with wich i've tried : 

Any suggestions is welcomed

```
ai_goals:
- set up a dev environnement for a react app using docker
- 'create a new bare react app '
- add a component to the react app that shows the current time
- run the app, access it and print in a file the html
ai_name: Developer-GPT
ai_role: Set up working coding a environment and then follow user instructions to
  create an app
```"
71,71,172,"Taking inspiration from the method that Langchain uses to instruct the bot on which tool to use, I've fleshed out each of the tools with a brief description to help the bot make a better decision. In testing it appears to work well and the bot seems to find its way to the goal faster."
72,72,171,What do you think of an option like this to enable language support ?
73,73,151,"Hi there! Looked around the open issues but haven't seen this suggestion yet

Firstly, I'd like to know that whatever is running is always logged (maybe that's addressed somewhere)

Secondly , I'd like a ""Cautious"" Monitored Continuous Mode
Where, a GPT 3.5 or other model, can see the live logs, and will trigger a PAUSE on the system for the following reasons:
- Exceeding Token Costs
- Runaway ML scenerios 
- Potentially Harmful/Immoral procedures 

I'd like to have some settings for time delays between executions or inside parts of the loop.
And ultimately the ability to Pause a run, Save it, (potentially edit its state while not running), and Resuming  the run finally. The pause can be triggered by the monitoring system, and resuming should only happen with user authorization.

Once this is a thing alerts would be the next step"
74,74,150,"## Summary

I propose a new feature called ""Semi-Active Mode,"" which enables the AI to run in a semi-automated manner, seeking user assistance when it encounters uncertainty, confusion, or ambiguity. This feature combines the benefits of Continuous Mode with the human-in-the-loop experience of Active Mode.

## Background

Currently, there are two modes available:

1. ""[Continuous Mode](https://github.com/Torantulino/Auto-GPT/#-continuous-mode-%EF%B8%8F),"" which allows the AI to run without user authorization and is 100% automated. However, this mode is not recommended due to its potential dangers, such as running indefinitely or performing actions that users might not approve.
2. ""[Active Mode](https://github.com/Torantulino/Auto-GPT/issues/13),"" which enables the AI to run while actively prompting the user with chain-of-thought questions when executing each subsequent action. This allows users to actively participate while the AI agent runs, ensuring a human-in-the-loop experience.

To further enhance user engagement and provide a more flexible experience, I propose a new feature called ""Semi-Active Mode.""

## Feature Description

In ""Semi-Active Mode,"" the AI will:

1. Execute an action.
2. Evaluate its confidence in the action or result.
3. If the confidence is below a certain threshold, prompt the user for assistance or clarification.
4. Incorporate the user's input and continue to the next action.

This interaction pattern allows users to assist when needed while still benefiting from the AI's capabilities. It strikes a balance between full automation and active participation, fostering a collaborative environment between the user and the AI system.

## Example Implementation

Here's an example implementation using LangChain's [Human-As-A-Tool](https://python.langchain.com/en/latest/modules/agents/tools/examples/human_tools.html) feature:

```python
import sys
from langchain.chat_models import ChatOpenAI
from langchain.llms import OpenAI
from langchain.agents import load_tools, initialize_agent
from langchain.agents.agent_types import AgentType

llm = ChatOpenAI(temperature=0.0)
math_llm = OpenAI(temperature=0.0)
tools = load_tools(
    [""human"", ""llm-math""], 
    llm=math_llm,
)

agent_chain = initialize_agent(
    tools,
    llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
)

agent_chain.run(""What is Eric Zhu's birthday?"")
```

In this code, the AI agent seeks human assistance when it encounters uncertainty, allowing the user to guide as needed.

## Benefits

-  Enhanced user engagement
-  Reduced risk of AI performing unwanted actions
-  Increased collaboration between the user and the AI
-  Balances automation and user control

## Risks and Mitigations

This feature may slow down the overall AI operation due to the need for user input in certain situations. However, this trade-off is acceptable, considering the increased control and collaboration it provides.

## Request for Comments

I would appreciate feedback from the community on this suggested feature. Please share your thoughts, suggestions, and any potential concerns you may have."
75,75,148,"Love the project, wondering if you'd like to use vocode's interfaces for speech synthesis!
- ports the existing ElevenLabs, gTTS, and StreamElements integrations
- enables Azure voices: https://azure.microsoft.com/en-us/products/cognitive-services/text-to-speech/

pros of using vocode:
- no need to save the interim synthesis file, all in memory
- one library for all TTS services; we'll add new synthesizers in new versions of vocode, so they'll be automatically enabled here!"
76,76,146,"Hey everyone, I've been working on integrating Redis and a separate terminal-based CLI client with AutoGPT to enable asynchronous question-and-answer user interactions. This is just a draft PR, so any feedback or contributions are more than welcome!

This will allow the agent to ask follow up questions based on long term work done on it's goals. Checking its work against the users wishes, etc. It also allows the system to set up user notifications.

Just like a person can have an inner monologue while they talk to a person, this allows chatgpt to interact with a human without impeding its ability to live and work asynchronously, as most of the other PR's about user QA do."
77,77,114,"GPT already knows way too much stuff, there's no need to search the web for every little sub-task that comes up. For example, if you ask GPT what the rules for texas hold'em poker are, it knows and is able to detail it without any issues. When using Auto-GPT, instead of using this already known knowledge, Auto-GPT enters an infinite loop of searching the web and getting 404s and looking for more links, etc. All of that is unnecessary.

So, maybe you should ask GPT if the information is already known BEFORE making the decision to search the web?"
78,78,85,"This PR focuses on enhancing the file operations module by implementing the following changes:

* Replacing the `os` and `os.path` modules with the `pathlib module`, which simplifies file and directory operations while providing better readability.
* Improving exception handling by using specific exceptions (e.g., FileNotFoundError) instead of generic exceptions where appropriate. This change enables more informative error messages to be returned to the user.
* Updating error message formatting by using f-strings, which enhances readability and consistency.


These updates contribute to a more robust and maintainable file operations module, making it easier to understand and modify in the future.

"
79,79,56,"Create a new command to dynamically expand capabilities.

1. Identify a capability gap. e.g. if current weather/stock prices cannot be obtained from current Auto-GPT capabilities 
2. Provide a static list of known API libraries (or discover via google search).
3. Find an API for the required capability.
4. Create a python command in-memory to call the discovered API schema.
5. Call a sub-agent to impersonate the in-memory API python command.
6. Ask the sub-agent to return a response for the new capability.

Some example API libraries:
- https://api.publicapis.org/entries
- https://api.m3o.com/v1/app/List
- https://github.com/public-apis/public-apis
- https://developers.google.com/apis-explorer
- https://learn.microsoft.com/en-us/python/api/?view=azure-python
- https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/index.html"
80,80,38,"AutoGPT keeps exceeding the token limit by like 100 tokens every time I use it before it finishes it's task and can't handle the error
Traceback (most recent call last):
  File ""/root/Auto-GPT/scripts/main.py"", line 199, in <module>
    assistant_reply = chat.chat_with_ai(
  File ""/root/Auto-GPT/scripts/chat.py"", line 64, in chat_with_ai
    response = openai.ChatCompletion.create(
  File ""/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_requestor.py"", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_requestor.py"", line 619, in _interpret_response
    self._interpret_response_line(
  File ""/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_requestor.py"", line 682, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 8215 tokens. Please reduce the length of the messages.

NEXT ACTION:  COMMAND = Error: ARGUMENTS = Invalid JSON
SYSTEM:  Command Error: returned: Unknown command Error:
Traceback (most recent call last):
  File ""/root/Auto-GPT/scripts/main.py"", line 199, in <module>
    assistant_reply = chat.chat_with_ai(
  File ""/root/Auto-GPT/scripts/chat.py"", line 64, in chat_with_ai
    response = openai.ChatCompletion.create(
  File ""/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py"", line 25, in create
    return super().create(*args, **kwargs)
  File ""/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py"", line 153, in create
    response, _, api_key = requestor.request(
  File ""/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_requestor.py"", line 226, in request
    resp, got_stream = self._interpret_response(result, stream)
  File ""/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_requestor.py"", line 619, in _interpret_response
    self._interpret_response_line(
  File ""/root/Auto-GPT/scripts/myenv/lib/python3.10/site-packages/openai/api_requestor.py"", line 682, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 8215 tokens. Please reduce the length of the messages.
(myenv) (base) root@DESKTOP-S70O6TN:~/Auto-GPT/scripts#"
81,81,25,"You can modify the code to accept a config file as input, and read the Chosen_Model flag to select the appropriate AI model. Here's an example of how to achieve this:

Create a sample config file named config.ini:

[AI]
Chosen_Model = gpt-4

Offload the call_ai_fuction from the ai_functions.py to a separate library. Modify the call_ai_function function to read the model from the config file:

```
import configparser

def call_ai_function(function, args, description, config_path=""config.ini""):
    # Load the configuration file
    config = configparser.ConfigParser()
    config.read(config_path)

    # Get the chosen model from the config file
    model = config.get(""AI"", ""Chosen_Model"", fallback=""gpt-4"")

    # Parse args to comma separated string
    args = "", "".join(args)
    messages = [
        {
            ""role"": ""system"",
            ""content"": f""You are now the following python function: ```# {description}\n{function}```\n\nOnly respond with your `return` value."",
        },
        {""role"": ""user"", ""content"": args},
    ]

  # Use different AI APIs based on the chosen model
    if model == ""gpt-4"":
        response = openai.ChatCompletion.create(
            model=model, messages=messages, temperature=0
        )
    elif model == ""some_other_api"":
        # Add code to call another AI API with the appropriate parameters
        response = some_other_api_call(parameters)
    else:
        raise ValueError(f""Unsupported model: {model}"")

    return response.choices[0].message[""content""]

```
In this modified version, the call_ai_function takes an additional parameter config_path which defaults to ""config.ini"". The function reads the config file, retrieves the Chosen_Model value, and uses it as the model for the OpenAI API call. If the Chosen_Model flag is not found in the config file, it defaults to ""gpt-4"".

 the if/elif structure is used to call different AI APIs based on the chosen model from the configuration file. Replace some_other_api with the name of the API you'd like to use, and replace parameters with the appropriate parameters required by that API. You can extend the if/elif structure to include more AI APIs as needed."
82,82,21,"It doesn't work?

<img width=""1440"" alt=""image"" src=""https://user-images.githubusercontent.com/6225438/229346042-a8687d08-8c4d-4094-8da4-b2300fb51732.png"">
"
83,83,15,"## Idea üí°
The **ULTIMATE** achievement for this project would be if Auto-GPT was able to recursively improve itself. That, after-all, is how AGI is predicted by many to come about. 

## Suggestion üë©‚Äçüíª
Auto-GPT should be able to:

- [ ] Read it's own code
- [ ] Evaluate it's limitations and areas for improvement
- [ ] Write code to increase it's abilities
- [ ] Write tests for it's code and carry out those tests

## Further down the line: üìÉ

- [ ] Browse it's own code on GitHub
- [ ] Evaluate, find bugs, etc
- [ ] Submit pull requests


## Where to start? ü§î
I have previously had success with this system prompt in playground:
![image](https://user-images.githubusercontent.com/22963551/229265447-ec41f8cf-19ac-4326-a67b-31d6422c6189.png)
<details><summary>Prompt</summary>
<p>
You are AGI_Builder_GPT. Your goal is to write code and make actionable suggestions in order to improve an AI called ""Auto-GPT"", so as to broaden the range of tasks it's capable of carrying out.
</p>
</details> "
84,84,11,"_Context:_  Implement  **reflection**, a technique that allows generating more coherent and natural texts using pre-trained language models.

_Problem or idea:_  Reflection is based on two articles that propose different methods to incorporate world knowledge and causal reasoning in text generation. The articles are:

1.  [ArXiv Article](https://arxiv.org/abs/2303.11366)
2.  [GitHub Repository](https://github.com/GammaTauAI/reflexion-human-eval)

_Solution or next step:_  I would like the  _Auto-GPT_  project to include reflection as an option to improve the quality of the generated texts. @Torantulino, what do you think of this idea?
"
85,85,6,"Auto-GPT is expensive to run due to GPT-4's API cost.

We could experiment with making it aware of this fact, by tracking tokens as they are used and converting to a dollar cost. 

This could also be displayed to the user to help them be more aware of exactly how much they are spending."
86,86,5,"**Auto-GPT currently pins it's Long-Term memory to the start of it's context window. It is able to manage this through commands.**

Auto-GPT should be aware of it's short and long term memory usage so that it knows when something is doing to be deleted from it's memory due to context limits. _e.g memory usage: (2555/4000 tokens)_

This may lead to some interesting behaviour where it is less inclined to read long strings of text, or is more meticulous at saving information to long-term-memory when it sees it's running low on tokens."
